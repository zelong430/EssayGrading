{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import Input,Activation,Conv1D,MaxPooling1D,Flatten,Dense,Embedding,LSTM,Merge, Dropout, TimeDistributedDense\n",
    "from keras.models import Model, Sequential\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.optimizers import SGD\n",
    "import evaluate\n",
    "from keras.regularizers import l2, activity_l2\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 300\n",
    "MAX_SEQUENCE_LENGTH = 1200\n",
    "MAX_NB_WORDS = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences_train = tokenizer.texts_to_sequences(texts_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 38025 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pad_sequences(sequences_train, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Shape of data tensor:', (10314, 1200))\n",
      "('Shape of label tensor:', (10314,))\n"
     ]
    }
   ],
   "source": [
    "labels = to_categorical(np.asarray(labels_train))\n",
    "print('Shape of data tensor:', data_train.shape)\n",
    "print('Shape of label tensor:', labels_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# split the data into a training set and a validation set\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "nb_validation_samples = int(0.8 * data.shape[0])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "x_train = data[:-nb_validation_samples]\n",
    "y_train = labels[:-nb_validation_samples]\n",
    "x_val = data[-nb_validation_samples:]\n",
    "y_val = labels[-nb_validation_samples:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400001 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "f = open(\"glove/glove.6B.300d.txt\", \"r\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "\n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "x = Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(128, 5, activation='relu')(x)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(128, 5, activation='relu')(x)\n",
    "x = MaxPooling1D(35)(x)  # global max pooling\n",
    "x = Flatten()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "preds = Dense(61, activation='softmax')(x)\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.engine.topology.InputLayer at 0x15fbde5d0>,\n",
       " <keras.layers.embeddings.Embedding at 0x17f2bba10>,\n",
       " <keras.layers.convolutional.Convolution1D at 0x1512e23d0>,\n",
       " <keras.layers.pooling.MaxPooling1D at 0x15fbde850>,\n",
       " <keras.layers.convolutional.Convolution1D at 0x15fc41a10>,\n",
       " <keras.layers.pooling.MaxPooling1D at 0x15fc41e50>,\n",
       " <keras.layers.convolutional.Convolution1D at 0x15fc635d0>,\n",
       " <keras.layers.pooling.MaxPooling1D at 0x15fc96d10>,\n",
       " <keras.layers.core.Flatten at 0x15fe06b10>,\n",
       " <keras.layers.core.Dense at 0x15fe23d50>,\n",
       " <keras.layers.core.Dense at 0x15fe4ee10>]"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2063 samples, validate on 8251 samples\n",
      "Epoch 1/2\n",
      "2063/2063 [==============================] - 109s - loss: 3.7060 - acc: 0.1677 - val_loss: 3.1452 - val_acc: 0.1128\n",
      "Epoch 2/2\n",
      "2063/2063 [==============================] - 104s - loss: 2.7153 - acc: 0.2113 - val_loss: 2.5326 - val_acc: 0.2165\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x137910490>"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# happy learning!\n",
    "model.fit(x_train, y_train, validation_data=(x_val, y_val),\n",
    "          nb_epoch=2, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred_y = model.predict(x_val, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Model' object has no attribute 'predict_classes'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-153-1d559e5a739d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'Model' object has no attribute 'predict_classes'"
     ]
    }
   ],
   "source": [
    "model.predict_classes(x_val, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(pred_y[7000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data and Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " 'i' in stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_data(text, keep_period = False):\n",
    "    text = text.lower()\n",
    "    NER_pat = re.compile(\"(@[a-z]+)[1-9]+\")\n",
    "    text = NER_pat.sub('\\\\1', text)\n",
    "\n",
    "    NUM_pat = re.compile(\"(\\d+)\\S*\")\n",
    "    text = NUM_pat.sub('@number', text)\n",
    "\n",
    "    if keep_period:\n",
    "        text = re.sub('[^a-zA-Z0-9.@]', ' ', text)\n",
    "    else:\n",
    "        text = re.sub('[^a-zA-Z0-9@]', ' ', text)\n",
    "    word_list = text.split()\n",
    "    filtered_words = [word for word in word_list if word not in stopwords.words('english')]\n",
    "    text = ' '.join(filtered_words)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('processed_data/train.tsv',delimiter='\\t')\n",
    "val = pd.read_csv('processed_data/val.tsv',delimiter='\\t')\n",
    "test = pd.read_csv('processed_data/test.tsv',delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = train[train[\"essay_set\"] != 8]\n",
    "test = test[test[\"essay_set\"] != 8]\n",
    "train = train[train[\"essay_set\"] != 7]\n",
    "test = test[test[\"essay_set\"] != 7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts_train = train[\"essay\"].values\n",
    "texts_train = [clean_data(i).replace('@', '') for i in texts_train]\n",
    "\n",
    "\n",
    "texts_val = val[\"essay\"].values\n",
    "texts_val = [clean_data(i).replace('@', '') for i in texts_val]\n",
    "\n",
    "texts_test = test[\"essay\"].values\n",
    "texts_test = [clean_data(i).replace('@', '') for i in texts_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels_train = train[\"domain1_score\"].values\n",
    "labels_val = val[\"domain1_score\"].values\n",
    "labels_test = test[\"domain1_score\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class w2v_Model(object):\n",
    "    def __init__(self, path):\n",
    "        self.path = path\n",
    "        self.vocab = {}\n",
    "        with open(self.path, 'r') as f:\n",
    "            for line in f:\n",
    "                values = line.split()\n",
    "                word = values[0]\n",
    "                coefs = np.asarray(values[1:], dtype='float32')\n",
    "                self.vocab[word] = coefs\n",
    "\n",
    "        print('Found %s word vectors.' % len(self.vocab))\n",
    "\n",
    "    def __getitem__(self, word):\n",
    "        '''\n",
    "        Return a word vector or subset of word vectors depending on the key.\n",
    "        If `key` is a word string, returns the word vector for that word if present,\n",
    "        else throws and error. If `key` is a list of word strings, returns a matrix X\n",
    "        where\n",
    "        \n",
    "            X[i] = word_vector(key[i])\n",
    "\n",
    "        else throws an error if any of the word strings does not have a vector.\n",
    "\n",
    "        Example:\n",
    "        m = Model('vectors.txt'); m.load()\n",
    "        m['hello'] # returns a single vector of shape (300,)\n",
    "        m[['hello', 'world']] # returns a matrix with shape (2, 300)\n",
    "        '''\n",
    "        \n",
    "        if isinstance(word, str):\n",
    "            return self.__safe_get__(word)\n",
    "        elif isinstance(word, list):\n",
    "            indices = map(lambda w: self.__safe_get__(w), word)\n",
    "            return np.array(indices)\n",
    "        \n",
    "    def __safe_get__(self, word):\n",
    "        if word in self.vocab:\n",
    "            return self.vocab[word]\n",
    "        else:\n",
    "            return np.zeros(EMBEDDING_DIM)\n",
    "    \n",
    "    def __contains__(self, word):\n",
    "        return word in self.vocab\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400001 word vectors.\n"
     ]
    }
   ],
   "source": [
    "w2v = w2v_Model('glove/glove.6B.100d.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple NN Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def regress_kappa(y_true, y_pred, target = train):\n",
    "    #y_pred = y_pred[:,0]\n",
    "    \n",
    "    result_y = np.zeros((len(y_pred),3))\n",
    "    result_y[:, 0 ] = target[\"essay_set\"].values\n",
    "    result_y[:,1] = target[\"essay_id\"].values\n",
    "    result_y[:,2] = np.round(y_pred[:,0])\n",
    "    result_y = result_y.astype('int64')\n",
    "    \n",
    "    true_y = np.zeros((len(y_true),3))\n",
    "    true_y[:, 0 ] = target[\"essay_set\"].values\n",
    "    true_y[:,1] = target[\"essay_id\"].values\n",
    "    true_y[:,2] = y_true \n",
    "    true_y = true_y.astype('int64')\n",
    "    \n",
    "    return evaluate.evaluate(result_y, true_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_x = np.array(map(lambda essay: w2v[essay.split()].mean(axis = 0), texts_train))\n",
    "train_y = labels_train.astype('float32')\n",
    "test_x = np.array(map(lambda essay: w2v[essay.split()].mean(axis = 0), texts_test))\n",
    "test_y = labels_test.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_x = np.hstack((train_x, to_categorical(train[\"essay_set\"].values)))\n",
    "test_x = np.hstack((test_x, to_categorical(test[\"essay_set\"].values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "Dst tensor is not initialized.\n\t [[Node: Const_369 = Const[dtype=DT_FLOAT, value=Tensor<type: float shape: [100,300] values: [0 0 0]...>, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\n\nCaused by op u'Const_369', defined at:\n  File \"/Users/zelongqiu/anaconda/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/Users/zelongqiu/anaconda/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/Users/zelongqiu/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/Users/zelongqiu/anaconda/lib/python2.7/site-packages/traitlets/config/application.py\", line 653, in launch_instance\n    app.start()\n  File \"/Users/zelongqiu/anaconda/lib/python2.7/site-packages/ipykernel/kernelapp.py\", line 474, in start\n    ioloop.IOLoop.instance().start()\n  File \"/Users/zelongqiu/anaconda/lib/python2.7/site-packages/zmq/eventloop/ioloop.py\", line 162, in start\n    super(ZMQIOLoop, self).start()\n  File \"/Users/zelongqiu/anaconda/lib/python2.7/site-packages/tornado/ioloop.py\", line 887, in start\n    handler_func(fd_obj, events)\n  File \"/Users/zelongqiu/anaconda/lib/python2.7/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/zelongqiu/anaconda/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/Users/zelongqiu/anaconda/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/Users/zelongqiu/anaconda/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/Users/zelongqiu/anaconda/lib/python2.7/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/zelongqiu/anaconda/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/Users/zelongqiu/anaconda/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/Users/zelongqiu/anaconda/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 390, in execute_request\n    user_expressions, allow_stdin)\n  File \"/Users/zelongqiu/anaconda/lib/python2.7/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/Users/zelongqiu/anaconda/lib/python2.7/site-packages/ipykernel/zmqshell.py\", line 501, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/Users/zelongqiu/anaconda/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Users/zelongqiu/anaconda/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2827, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/Users/zelongqiu/anaconda/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-28-ea0e2be225af>\", line 10, in <module>\n    model.fit(train_x, train_y, batch_size = 64, nb_epoch = 300 )\n  File \"/Users/zelongqiu/anaconda/lib/python2.7/site-packages/keras/models.py\", line 652, in fit\n    sample_weight=sample_weight)\n  File \"/Users/zelongqiu/anaconda/lib/python2.7/site-packages/keras/engine/training.py\", line 1083, in fit\n    self._make_train_function()\n  File \"/Users/zelongqiu/anaconda/lib/python2.7/site-packages/keras/engine/training.py\", line 696, in _make_train_function\n    self.total_loss)\n  File \"/Users/zelongqiu/anaconda/lib/python2.7/site-packages/keras/optimizers.py\", line 387, in get_updates\n    ms = [K.zeros(shape) for shape in shapes]\n  File \"/Users/zelongqiu/anaconda/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py\", line 277, in zeros\n    return variable(tf.constant_initializer(0., dtype=tf_dtype)(shape),\n  File \"/Users/zelongqiu/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/init_ops.py\", line 149, in _initializer\n    return constant_op.constant(value, dtype=dtype, shape=shape)\n  File \"/Users/zelongqiu/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/constant_op.py\", line 169, in constant\n    attrs={\"value\": tensor_value, \"dtype\": dtype_value}, name=name).outputs[0]\n  File \"/Users/zelongqiu/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2240, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/Users/zelongqiu/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1128, in __init__\n    self._traceback = _extract_stack()\n\nInternalError (see above for traceback): Dst tensor is not initialized.\n\t [[Node: Const_369 = Const[dtype=DT_FLOAT, value=Tensor<type: float shape: [100,300] values: [0 0 0]...>, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-ea0e2be225af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mean_squared_error'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'mean_absolute_error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m300\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/zelongqiu/anaconda/lib/python2.7/site-packages/keras/models.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, **kwargs)\u001b[0m\n\u001b[1;32m    650\u001b[0m                               \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 652\u001b[0;31m                               sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    653\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m/Users/zelongqiu/anaconda/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch)\u001b[0m\n\u001b[1;32m   1109\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1110\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1111\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/zelongqiu/anaconda/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, nb_epoch, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[1;32m    824\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 826\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    827\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    828\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/zelongqiu/anaconda/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1093\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse_coo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse_coo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1094\u001b[0m             \u001b[0mfeed_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1095\u001b[0;31m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1096\u001b[0m         \u001b[0mupdated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdates_op\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1097\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/zelongqiu/anaconda/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36mget_session\u001b[0;34m()\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_SESSION\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_MANUAL_VAR_INIT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0m_initialize_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/zelongqiu/anaconda/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m_initialize_variables\u001b[0;34m()\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0msess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'variables_initializer'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m             \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muninitialized_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m             \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muninitialized_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/zelongqiu/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 766\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    767\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/zelongqiu/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    962\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 964\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    965\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/zelongqiu/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1014\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1015\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/Users/zelongqiu/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1032\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1034\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1035\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInternalError\u001b[0m: Dst tensor is not initialized.\n\t [[Node: Const_369 = Const[dtype=DT_FLOAT, value=Tensor<type: float shape: [100,300] values: [0 0 0]...>, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\n\nCaused by op u'Const_369', defined at:\n  File \"/Users/zelongqiu/anaconda/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/Users/zelongqiu/anaconda/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/Users/zelongqiu/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/Users/zelongqiu/anaconda/lib/python2.7/site-packages/traitlets/config/application.py\", line 653, in launch_instance\n    app.start()\n  File \"/Users/zelongqiu/anaconda/lib/python2.7/site-packages/ipykernel/kernelapp.py\", line 474, in start\n    ioloop.IOLoop.instance().start()\n  File \"/Users/zelongqiu/anaconda/lib/python2.7/site-packages/zmq/eventloop/ioloop.py\", line 162, in start\n    super(ZMQIOLoop, self).start()\n  File \"/Users/zelongqiu/anaconda/lib/python2.7/site-packages/tornado/ioloop.py\", line 887, in start\n    handler_func(fd_obj, events)\n  File \"/Users/zelongqiu/anaconda/lib/python2.7/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/zelongqiu/anaconda/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/Users/zelongqiu/anaconda/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/Users/zelongqiu/anaconda/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/Users/zelongqiu/anaconda/lib/python2.7/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/zelongqiu/anaconda/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/Users/zelongqiu/anaconda/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/Users/zelongqiu/anaconda/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 390, in execute_request\n    user_expressions, allow_stdin)\n  File \"/Users/zelongqiu/anaconda/lib/python2.7/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/Users/zelongqiu/anaconda/lib/python2.7/site-packages/ipykernel/zmqshell.py\", line 501, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/Users/zelongqiu/anaconda/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Users/zelongqiu/anaconda/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2827, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/Users/zelongqiu/anaconda/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-28-ea0e2be225af>\", line 10, in <module>\n    model.fit(train_x, train_y, batch_size = 64, nb_epoch = 300 )\n  File \"/Users/zelongqiu/anaconda/lib/python2.7/site-packages/keras/models.py\", line 652, in fit\n    sample_weight=sample_weight)\n  File \"/Users/zelongqiu/anaconda/lib/python2.7/site-packages/keras/engine/training.py\", line 1083, in fit\n    self._make_train_function()\n  File \"/Users/zelongqiu/anaconda/lib/python2.7/site-packages/keras/engine/training.py\", line 696, in _make_train_function\n    self.total_loss)\n  File \"/Users/zelongqiu/anaconda/lib/python2.7/site-packages/keras/optimizers.py\", line 387, in get_updates\n    ms = [K.zeros(shape) for shape in shapes]\n  File \"/Users/zelongqiu/anaconda/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py\", line 277, in zeros\n    return variable(tf.constant_initializer(0., dtype=tf_dtype)(shape),\n  File \"/Users/zelongqiu/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/init_ops.py\", line 149, in _initializer\n    return constant_op.constant(value, dtype=dtype, shape=shape)\n  File \"/Users/zelongqiu/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/constant_op.py\", line 169, in constant\n    attrs={\"value\": tensor_value, \"dtype\": dtype_value}, name=name).outputs[0]\n  File \"/Users/zelongqiu/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2240, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/Users/zelongqiu/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1128, in __init__\n    self._traceback = _extract_stack()\n\nInternalError (see above for traceback): Dst tensor is not initialized.\n\t [[Node: Const_369 = Const[dtype=DT_FLOAT, value=Tensor<type: float shape: [100,300] values: [0 0 0]...>, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(300, input_dim=100, init='normal', activation='relu',\\\n",
    "          W_regularizer=l2(0.0001), activity_regularizer=activity_l2(0.0001)))\n",
    "model.add(Dense(150, init='normal', activation='relu',\\\n",
    "          W_regularizer=l2(0.0001), activity_regularizer=activity_l2(0.0001)))\n",
    "model.add(Dense(1, init='normal'))\n",
    "\n",
    "sgd = SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "\n",
    "\n",
    "model.compile(loss='mean_squared_error', metrics = ['mean_absolute_error'], optimizer='adam')\n",
    "model.fit(train_x, train_y, batch_size = 64, nb_epoch = 300 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>essay_set</th>\n",
       "      <th>essay</th>\n",
       "      <th>rater1_domain1</th>\n",
       "      <th>rater2_domain1</th>\n",
       "      <th>domain1_score</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>2979</td>\n",
       "      <td>2</td>\n",
       "      <td>Write a persuasive essay to a newspaper reflec...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1.645708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>2984</td>\n",
       "      <td>2</td>\n",
       "      <td>How @CAPS4 you feel if your favorite book was ...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4.328355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>2996</td>\n",
       "      <td>2</td>\n",
       "      <td>If the people that are publishing and writing ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3.305116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>3029</td>\n",
       "      <td>2</td>\n",
       "      <td>wow thats racist. as i said when i saw the mov...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2.887580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>3033</td>\n",
       "      <td>2</td>\n",
       "      <td>Why should we be more carefull of what we get ...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3.106670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>3045</td>\n",
       "      <td>2</td>\n",
       "      <td>Should books, magazines, music, movies, and ec...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3.428966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>3099</td>\n",
       "      <td>2</td>\n",
       "      <td>Twilight, @PERSON1, or even The @CAPS1 @CAPS2 ...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4.173596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>3113</td>\n",
       "      <td>2</td>\n",
       "      <td>I dont believe books should be takin off of th...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3.787910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>3123</td>\n",
       "      <td>2</td>\n",
       "      <td>I believe censorship should be used in librari...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3.740323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>3126</td>\n",
       "      <td>2</td>\n",
       "      <td>Real life is rated @CAPS1 so why pretend it's ...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3.855362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>3155</td>\n",
       "      <td>2</td>\n",
       "      <td>The censorship in libraries should be as gog a...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2.965737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>3169</td>\n",
       "      <td>2</td>\n",
       "      <td>'And then we have no books left on the shelf f...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3.972822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>3189</td>\n",
       "      <td>2</td>\n",
       "      <td>Books have been apart of every single person's...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3.613929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>3192</td>\n",
       "      <td>2</td>\n",
       "      <td>When I go to a library I @MONTH1 find some stu...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3.438865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>3208</td>\n",
       "      <td>2</td>\n",
       "      <td>If I could remove anything from shelves of lib...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3.619192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>3219</td>\n",
       "      <td>2</td>\n",
       "      <td>I believe that libraries should provide any ki...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3.713111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>3226</td>\n",
       "      <td>2</td>\n",
       "      <td>Many places show and sell offensive material. ...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3.686349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>3243</td>\n",
       "      <td>2</td>\n",
       "      <td>Have you ever walked into the library hoping t...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3.858116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>3262</td>\n",
       "      <td>2</td>\n",
       "      <td>How many of you have ever heard any offensive ...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3.546199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>3265</td>\n",
       "      <td>2</td>\n",
       "      <td>A very wise man in @CAPS1 life once said, 'The...</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4.390672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>3269</td>\n",
       "      <td>2</td>\n",
       "      <td>Typically when you walk in to a library there ...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3.699503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>3285</td>\n",
       "      <td>2</td>\n",
       "      <td>I think that if certain materials are offensiv...</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2.806852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>3293</td>\n",
       "      <td>2</td>\n",
       "      <td>Recently the growing issue of censorship in li...</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3.738950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>3294</td>\n",
       "      <td>2</td>\n",
       "      <td>I believe that if you hear or read something o...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3.444291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>3297</td>\n",
       "      <td>2</td>\n",
       "      <td>There are many books that are not appropriate ...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3.019498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>3298</td>\n",
       "      <td>2</td>\n",
       "      <td>Many people base their actions on their own vi...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4.076538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>3299</td>\n",
       "      <td>2</td>\n",
       "      <td>Many libraries contain books, movies, music, a...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4.264161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>3303</td>\n",
       "      <td>2</td>\n",
       "      <td>I'm writing this paper today to talk about cen...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2.676398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>3349</td>\n",
       "      <td>2</td>\n",
       "      <td>There are tons of books in the library that pe...</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3.576715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>3350</td>\n",
       "      <td>2</td>\n",
       "      <td>I don't think that they should be removed from...</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3.273365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>4527</td>\n",
       "      <td>2</td>\n",
       "      <td>Today it seems as if anything you could imagin...</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3.847282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>4543</td>\n",
       "      <td>2</td>\n",
       "      <td>People @MONTH1 have a certain reason to take s...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3.896544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>4557</td>\n",
       "      <td>2</td>\n",
       "      <td>Censorship in media has been around since the ...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3.128128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>4564</td>\n",
       "      <td>2</td>\n",
       "      <td>Everyone has there own opinions and arguements...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3.480490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>4574</td>\n",
       "      <td>2</td>\n",
       "      <td>Where it @CAPS1 @CAPS2 @CAPS3     @CAPS4 up no...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3.858097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>4575</td>\n",
       "      <td>2</td>\n",
       "      <td>Censorship is a two sided question. There are ...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4.013980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>4577</td>\n",
       "      <td>2</td>\n",
       "      <td>There are many things out in the 'material' wo...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3.656914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>4578</td>\n",
       "      <td>2</td>\n",
       "      <td>Controversy in the @CAPS1          @CAPS2 some...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4.293631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>4583</td>\n",
       "      <td>2</td>\n",
       "      <td>Nobody should believe that we have the choice ...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3.389773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>4597</td>\n",
       "      <td>2</td>\n",
       "      <td>BAD @CAPS1     Do you really want your kids to...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3.173028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>4602</td>\n",
       "      <td>2</td>\n",
       "      <td>How many of you out there absolutely love to r...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4.052299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>4607</td>\n",
       "      <td>2</td>\n",
       "      <td>Do yo think censorship is a good thing? Do you...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3.067852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>4617</td>\n",
       "      <td>2</td>\n",
       "      <td>Everyone has found something offensive through...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3.388415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>4626</td>\n",
       "      <td>2</td>\n",
       "      <td>Reading is something people choose to do for t...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3.332023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>4641</td>\n",
       "      <td>2</td>\n",
       "      <td>I think if its offensive it should not be avav...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2.602835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>4642</td>\n",
       "      <td>2</td>\n",
       "      <td>Dear Newspaper,     I would like to start off ...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3.317338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>4643</td>\n",
       "      <td>2</td>\n",
       "      <td>I am sending this letter to you about my views...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3.052935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>4652</td>\n",
       "      <td>2</td>\n",
       "      <td>Censorship to most people means much more than...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3.556133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311</th>\n",
       "      <td>4657</td>\n",
       "      <td>2</td>\n",
       "      <td>Some children take books off the shelf that so...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.887913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312</th>\n",
       "      <td>4658</td>\n",
       "      <td>2</td>\n",
       "      <td>Hello @ORGANIZATION1, I am writing to talk abo...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2.576089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313</th>\n",
       "      <td>4670</td>\n",
       "      <td>2</td>\n",
       "      <td>Yes I do believe that certain books, movies, m...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3.584176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314</th>\n",
       "      <td>4680</td>\n",
       "      <td>2</td>\n",
       "      <td>Censorship in Libraries     Have you ever came...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3.636094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315</th>\n",
       "      <td>4681</td>\n",
       "      <td>2</td>\n",
       "      <td>I personally do not think that books or magazi...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3.373170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>4696</td>\n",
       "      <td>2</td>\n",
       "      <td>Hi, my name is mason and i'm writing about wet...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3.041039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>4697</td>\n",
       "      <td>2</td>\n",
       "      <td>I am a student from @CAPS1. I wanted to let yo...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3.653813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>4705</td>\n",
       "      <td>2</td>\n",
       "      <td>Certain material that are made to offend peopl...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3.072201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>4717</td>\n",
       "      <td>2</td>\n",
       "      <td>I think that no books should be taken away. I ...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3.041160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>4722</td>\n",
       "      <td>2</td>\n",
       "      <td>In our socioty, a lot of people and places tak...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3.312390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321</th>\n",
       "      <td>4728</td>\n",
       "      <td>2</td>\n",
       "      <td>I do not think that there is a need to remove ...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2.877138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322</th>\n",
       "      <td>4751</td>\n",
       "      <td>2</td>\n",
       "      <td>I do not believe that certain materials, such ...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4.113070</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>169 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     essay_id  essay_set                                              essay  \\\n",
       "154      2979          2  Write a persuasive essay to a newspaper reflec...   \n",
       "155      2984          2  How @CAPS4 you feel if your favorite book was ...   \n",
       "156      2996          2  If the people that are publishing and writing ...   \n",
       "157      3029          2  wow thats racist. as i said when i saw the mov...   \n",
       "158      3033          2  Why should we be more carefull of what we get ...   \n",
       "159      3045          2  Should books, magazines, music, movies, and ec...   \n",
       "160      3099          2  Twilight, @PERSON1, or even The @CAPS1 @CAPS2 ...   \n",
       "161      3113          2  I dont believe books should be takin off of th...   \n",
       "162      3123          2  I believe censorship should be used in librari...   \n",
       "163      3126          2  Real life is rated @CAPS1 so why pretend it's ...   \n",
       "164      3155          2  The censorship in libraries should be as gog a...   \n",
       "165      3169          2  'And then we have no books left on the shelf f...   \n",
       "166      3189          2  Books have been apart of every single person's...   \n",
       "167      3192          2  When I go to a library I @MONTH1 find some stu...   \n",
       "168      3208          2  If I could remove anything from shelves of lib...   \n",
       "169      3219          2  I believe that libraries should provide any ki...   \n",
       "170      3226          2  Many places show and sell offensive material. ...   \n",
       "171      3243          2  Have you ever walked into the library hoping t...   \n",
       "172      3262          2  How many of you have ever heard any offensive ...   \n",
       "173      3265          2  A very wise man in @CAPS1 life once said, 'The...   \n",
       "174      3269          2  Typically when you walk in to a library there ...   \n",
       "175      3285          2  I think that if certain materials are offensiv...   \n",
       "176      3293          2  Recently the growing issue of censorship in li...   \n",
       "177      3294          2  I believe that if you hear or read something o...   \n",
       "178      3297          2  There are many books that are not appropriate ...   \n",
       "179      3298          2  Many people base their actions on their own vi...   \n",
       "180      3299          2  Many libraries contain books, movies, music, a...   \n",
       "181      3303          2  I'm writing this paper today to talk about cen...   \n",
       "182      3349          2  There are tons of books in the library that pe...   \n",
       "183      3350          2  I don't think that they should be removed from...   \n",
       "..        ...        ...                                                ...   \n",
       "293      4527          2  Today it seems as if anything you could imagin...   \n",
       "294      4543          2  People @MONTH1 have a certain reason to take s...   \n",
       "295      4557          2  Censorship in media has been around since the ...   \n",
       "296      4564          2  Everyone has there own opinions and arguements...   \n",
       "297      4574          2  Where it @CAPS1 @CAPS2 @CAPS3     @CAPS4 up no...   \n",
       "298      4575          2  Censorship is a two sided question. There are ...   \n",
       "299      4577          2  There are many things out in the 'material' wo...   \n",
       "300      4578          2  Controversy in the @CAPS1          @CAPS2 some...   \n",
       "301      4583          2  Nobody should believe that we have the choice ...   \n",
       "302      4597          2  BAD @CAPS1     Do you really want your kids to...   \n",
       "303      4602          2  How many of you out there absolutely love to r...   \n",
       "304      4607          2  Do yo think censorship is a good thing? Do you...   \n",
       "305      4617          2  Everyone has found something offensive through...   \n",
       "306      4626          2  Reading is something people choose to do for t...   \n",
       "307      4641          2  I think if its offensive it should not be avav...   \n",
       "308      4642          2  Dear Newspaper,     I would like to start off ...   \n",
       "309      4643          2  I am sending this letter to you about my views...   \n",
       "310      4652          2  Censorship to most people means much more than...   \n",
       "311      4657          2  Some children take books off the shelf that so...   \n",
       "312      4658          2  Hello @ORGANIZATION1, I am writing to talk abo...   \n",
       "313      4670          2  Yes I do believe that certain books, movies, m...   \n",
       "314      4680          2  Censorship in Libraries     Have you ever came...   \n",
       "315      4681          2  I personally do not think that books or magazi...   \n",
       "316      4696          2  Hi, my name is mason and i'm writing about wet...   \n",
       "317      4697          2  I am a student from @CAPS1. I wanted to let yo...   \n",
       "318      4705          2  Certain material that are made to offend peopl...   \n",
       "319      4717          2  I think that no books should be taken away. I ...   \n",
       "320      4722          2  In our socioty, a lot of people and places tak...   \n",
       "321      4728          2  I do not think that there is a need to remove ...   \n",
       "322      4751          2  I do not believe that certain materials, such ...   \n",
       "\n",
       "     rater1_domain1  rater2_domain1  domain1_score  prediction  \n",
       "154               1               2              1    1.645708  \n",
       "155               5               5              5    4.328355  \n",
       "156               1               1              1    3.305116  \n",
       "157               3               3              3    2.887580  \n",
       "158               4               4              4    3.106670  \n",
       "159               4               4              4    3.428966  \n",
       "160               4               4              4    4.173596  \n",
       "161               3               3              3    3.787910  \n",
       "162               4               4              4    3.740323  \n",
       "163               4               4              4    3.855362  \n",
       "164               3               3              3    2.965737  \n",
       "165               3               3              3    3.972822  \n",
       "166               4               4              4    3.613929  \n",
       "167               3               3              3    3.438865  \n",
       "168               4               4              4    3.619192  \n",
       "169               4               4              4    3.713111  \n",
       "170               4               4              4    3.686349  \n",
       "171               4               4              4    3.858116  \n",
       "172               4               4              4    3.546199  \n",
       "173               4               5              4    4.390672  \n",
       "174               4               4              4    3.699503  \n",
       "175               4               3              4    2.806852  \n",
       "176               4               5              4    3.738950  \n",
       "177               4               4              4    3.444291  \n",
       "178               3               3              3    3.019498  \n",
       "179               4               4              4    4.076538  \n",
       "180               4               4              4    4.264161  \n",
       "181               3               3              3    2.676398  \n",
       "182               3               4              3    3.576715  \n",
       "183               3               4              3    3.273365  \n",
       "..              ...             ...            ...         ...  \n",
       "293               4               3              4    3.847282  \n",
       "294               4               4              4    3.896544  \n",
       "295               4               4              4    3.128128  \n",
       "296               3               3              3    3.480490  \n",
       "297               4               4              4    3.858097  \n",
       "298               4               4              4    4.013980  \n",
       "299               5               5              5    3.656914  \n",
       "300               4               4              4    4.293631  \n",
       "301               3               3              3    3.389773  \n",
       "302               3               3              3    3.173028  \n",
       "303               4               4              4    4.052299  \n",
       "304               3               3              3    3.067852  \n",
       "305               4               4              4    3.388415  \n",
       "306               4               4              4    3.332023  \n",
       "307               3               3              3    2.602835  \n",
       "308               3               3              3    3.317338  \n",
       "309               3               3              3    3.052935  \n",
       "310               4               4              4    3.556133  \n",
       "311               1               1              1    0.887913  \n",
       "312               3               3              3    2.576089  \n",
       "313               3               3              3    3.584176  \n",
       "314               4               4              4    3.636094  \n",
       "315               3               3              3    3.373170  \n",
       "316               3               3              3    3.041039  \n",
       "317               4               4              4    3.653813  \n",
       "318               4               4              4    3.072201  \n",
       "319               2               2              2    3.041160  \n",
       "320               2               2              2    3.312390  \n",
       "321               4               4              4    2.877138  \n",
       "322               4               4              4    4.113070  \n",
       "\n",
       "[169 rows x 7 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_y = model.predict(test_x)\n",
    "test[\"prediction\"] = pred_y\n",
    "test[test[\"essay_set\"] == 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result_y = np.zeros((len(pred_y),3))\n",
    "result_y[:, 0 ] = test[\"essay_set\"].values\n",
    "result_y[:,1] = test[\"essay_id\"].values\n",
    "result_y[:,2] = np.round(pred_y[:,0])\n",
    "result_y = result_y.astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "true_y = np.zeros((len(pred_y),3))\n",
    "true_y[:, 0 ] = test[\"essay_set\"].values\n",
    "true_y[:,1] = test[\"essay_id\"].values\n",
    "true_y[:,2] = labels_test\n",
    "true_y = true_y.astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kappa for each set [ 0.65595079  0.60566842  0.42155585  0.68330306  0.62259739  0.67250027\n",
      "  0.64822492  0.6039781 ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.61912948702497483"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate.evaluate(result_y, true_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kappa for each set [ 0.92041811  0.76767925  0.92922567  0.9326163   0.94218313  0.92450137\n",
      "  0.98358915  0.9679918 ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.93801721836579621"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regress_kappa(labels_train,model.predict(train_x),train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple NN Softmax classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def one_hot_kappa(y_true, y_pred, target = train):\n",
    "    y_pred = np.argmax(y_pred,axis =1)\n",
    "    y_true = np.argmax(y_true,axis = 1)\n",
    "    \n",
    "    result_y = np.zeros((len(y_pred),3))\n",
    "    result_y[:, 0 ] = target[\"essay_set\"].values\n",
    "    result_y[:,1] = target[\"essay_id\"].values\n",
    "    result_y[:,2] = y_pred\n",
    "    result_y = result_y.astype('int64')\n",
    "    \n",
    "    true_y = np.zeros((len(y_true),3))\n",
    "    true_y[:, 0 ] = target[\"essay_set\"].values\n",
    "    true_y[:,1] = target[\"essay_id\"].values\n",
    "    true_y[:,2] = y_true \n",
    "    true_y = true_y.astype('int64')\n",
    "    \n",
    "    return evaluate.evaluate(result_y, true_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_x = np.array(map(lambda essay: w2v[essay.split()].mean(axis = 0), texts_train))\n",
    "test_x = np.array(map(lambda essay: w2v[essay.split()].mean(axis = 0), texts_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_x = np.hstack((train_x, to_categorical(train[\"essay_set\"].values)))\n",
    "test_x = np.hstack((test_x, to_categorical(test[\"essay_set\"].values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_y = to_categorical(labels_train)\n",
    "test_y = to_categorical(labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_y = to_categorical(np.hstack((labels_train, labels_test)))[0:len(labels_train),:]\n",
    "test_y = to_categorical(np.hstack((labels_train, labels_test)))[len(labels_train):,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "10314/10314 [==============================] - 1s - loss: 2.5369 - acc: 0.2342     \n",
      "Epoch 2/200\n",
      "10314/10314 [==============================] - 0s - loss: 1.7946 - acc: 0.3564     \n",
      "Epoch 3/200\n",
      "10314/10314 [==============================] - 0s - loss: 1.6819 - acc: 0.3788     \n",
      "Epoch 4/200\n",
      "10314/10314 [==============================] - 0s - loss: 1.6137 - acc: 0.3938     \n",
      "Epoch 5/200\n",
      "10314/10314 [==============================] - 0s - loss: 1.5468 - acc: 0.4108     \n",
      "Epoch 6/200\n",
      "10314/10314 [==============================] - 0s - loss: 1.4992 - acc: 0.4252     \n",
      "Epoch 7/200\n",
      "10314/10314 [==============================] - 0s - loss: 1.4600 - acc: 0.4403     \n",
      "Epoch 8/200\n",
      "10314/10314 [==============================] - 0s - loss: 1.4353 - acc: 0.4484     \n",
      "Epoch 9/200\n",
      "10314/10314 [==============================] - 0s - loss: 1.4128 - acc: 0.4498     \n",
      "Epoch 10/200\n",
      "10314/10314 [==============================] - 0s - loss: 1.3929 - acc: 0.4595     \n",
      "Epoch 11/200\n",
      "10314/10314 [==============================] - 0s - loss: 1.3675 - acc: 0.4683     \n",
      "Epoch 12/200\n",
      "10314/10314 [==============================] - 0s - loss: 1.3554 - acc: 0.4710     \n",
      "Epoch 13/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.3411 - acc: 0.4698     \n",
      "Epoch 14/200\n",
      "10314/10314 [==============================] - 0s - loss: 1.3236 - acc: 0.4793     \n",
      "Epoch 15/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.3129 - acc: 0.4875     \n",
      "Epoch 16/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.3132 - acc: 0.4860     \n",
      "Epoch 17/200\n",
      "10314/10314 [==============================] - 0s - loss: 1.2838 - acc: 0.4980     \n",
      "Epoch 18/200\n",
      "10314/10314 [==============================] - 0s - loss: 1.2736 - acc: 0.4996     \n",
      "Epoch 19/200\n",
      "10314/10314 [==============================] - 0s - loss: 1.2596 - acc: 0.5030     \n",
      "Epoch 20/200\n",
      "10314/10314 [==============================] - 0s - loss: 1.2468 - acc: 0.5124     \n",
      "Epoch 21/200\n",
      "10314/10314 [==============================] - 0s - loss: 1.2370 - acc: 0.5146     \n",
      "Epoch 22/200\n",
      "10314/10314 [==============================] - 0s - loss: 1.2364 - acc: 0.5169     \n",
      "Epoch 23/200\n",
      "10314/10314 [==============================] - 0s - loss: 1.2221 - acc: 0.5212     \n",
      "Epoch 24/200\n",
      "10314/10314 [==============================] - 0s - loss: 1.2054 - acc: 0.5269     \n",
      "Epoch 25/200\n",
      "10314/10314 [==============================] - 0s - loss: 1.1994 - acc: 0.5283     \n",
      "Epoch 26/200\n",
      "10314/10314 [==============================] - 0s - loss: 1.1831 - acc: 0.5390     \n",
      "Epoch 27/200\n",
      "10314/10314 [==============================] - 0s - loss: 1.1823 - acc: 0.5345     \n",
      "Epoch 28/200\n",
      "10314/10314 [==============================] - 0s - loss: 1.1708 - acc: 0.5322     \n",
      "Epoch 29/200\n",
      "10314/10314 [==============================] - 0s - loss: 1.1654 - acc: 0.5377     \n",
      "Epoch 30/200\n",
      "10314/10314 [==============================] - 0s - loss: 1.1428 - acc: 0.5505     \n",
      "Epoch 31/200\n",
      "10314/10314 [==============================] - 0s - loss: 1.1440 - acc: 0.5522     \n",
      "Epoch 32/200\n",
      "10314/10314 [==============================] - 0s - loss: 1.1399 - acc: 0.5504     \n",
      "Epoch 33/200\n",
      "10314/10314 [==============================] - 0s - loss: 1.1275 - acc: 0.5579     \n",
      "Epoch 34/200\n",
      "10314/10314 [==============================] - 0s - loss: 1.1237 - acc: 0.5556     \n",
      "Epoch 35/200\n",
      "10314/10314 [==============================] - 0s - loss: 1.1044 - acc: 0.5656     \n",
      "Epoch 36/200\n",
      "10314/10314 [==============================] - 0s - loss: 1.1009 - acc: 0.5675     \n",
      "Epoch 37/200\n",
      "10314/10314 [==============================] - 0s - loss: 1.0954 - acc: 0.5680     \n",
      "Epoch 38/200\n",
      "10314/10314 [==============================] - 0s - loss: 1.0781 - acc: 0.5776     \n",
      "Epoch 39/200\n",
      "10314/10314 [==============================] - 0s - loss: 1.0682 - acc: 0.5835     \n",
      "Epoch 40/200\n",
      "10314/10314 [==============================] - 0s - loss: 1.0695 - acc: 0.5808     \n",
      "Epoch 41/200\n",
      "10314/10314 [==============================] - 0s - loss: 1.0543 - acc: 0.5860     \n",
      "Epoch 42/200\n",
      "10314/10314 [==============================] - 0s - loss: 1.0435 - acc: 0.5879     \n",
      "Epoch 43/200\n",
      "10314/10314 [==============================] - 0s - loss: 1.0333 - acc: 0.5935     \n",
      "Epoch 44/200\n",
      "10314/10314 [==============================] - 0s - loss: 1.0330 - acc: 0.5979     \n",
      "Epoch 45/200\n",
      "10314/10314 [==============================] - 0s - loss: 1.0272 - acc: 0.5976     \n",
      "Epoch 46/200\n",
      "10314/10314 [==============================] - 0s - loss: 1.0178 - acc: 0.6035     \n",
      "Epoch 47/200\n",
      "10314/10314 [==============================] - 0s - loss: 1.0091 - acc: 0.6073     \n",
      "Epoch 48/200\n",
      "10314/10314 [==============================] - 0s - loss: 1.0057 - acc: 0.6072     \n",
      "Epoch 49/200\n",
      "10314/10314 [==============================] - 0s - loss: 1.0003 - acc: 0.6097     \n",
      "Epoch 50/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.9786 - acc: 0.6160     \n",
      "Epoch 51/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.9698 - acc: 0.6186     \n",
      "Epoch 52/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.9539 - acc: 0.6287     \n",
      "Epoch 53/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.9715 - acc: 0.6188     \n",
      "Epoch 54/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.9511 - acc: 0.6284     \n",
      "Epoch 55/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.9468 - acc: 0.6311     \n",
      "Epoch 56/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.9342 - acc: 0.6346     \n",
      "Epoch 57/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.9292 - acc: 0.6386     \n",
      "Epoch 58/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.9172 - acc: 0.6377     \n",
      "Epoch 59/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.8967 - acc: 0.6557     \n",
      "Epoch 60/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.8997 - acc: 0.6503     \n",
      "Epoch 61/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.8896 - acc: 0.6577     \n",
      "Epoch 62/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.8650 - acc: 0.6655     \n",
      "Epoch 63/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.8645 - acc: 0.6638     \n",
      "Epoch 64/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.8579 - acc: 0.6697     \n",
      "Epoch 65/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.8568 - acc: 0.6655     \n",
      "Epoch 66/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.8436 - acc: 0.6786     \n",
      "Epoch 67/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.8233 - acc: 0.6805     \n",
      "Epoch 68/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.8279 - acc: 0.6837     \n",
      "Epoch 69/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.8175 - acc: 0.6843     \n",
      "Epoch 70/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.8160 - acc: 0.6823     \n",
      "Epoch 71/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.8031 - acc: 0.6903     \n",
      "Epoch 72/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.7856 - acc: 0.6971     \n",
      "Epoch 73/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.7729 - acc: 0.7014     \n",
      "Epoch 74/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.7775 - acc: 0.6959     \n",
      "Epoch 75/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.7723 - acc: 0.7093     \n",
      "Epoch 76/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.7549 - acc: 0.7134     \n",
      "Epoch 77/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.7438 - acc: 0.7185     \n",
      "Epoch 78/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.7576 - acc: 0.7107     \n",
      "Epoch 79/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.7346 - acc: 0.7164     \n",
      "Epoch 80/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.7288 - acc: 0.7191     \n",
      "Epoch 81/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.7133 - acc: 0.7290     \n",
      "Epoch 82/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.6926 - acc: 0.7417     \n",
      "Epoch 83/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.6849 - acc: 0.7383     \n",
      "Epoch 84/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.6878 - acc: 0.7389     \n",
      "Epoch 85/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.6851 - acc: 0.7378     \n",
      "Epoch 86/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.6627 - acc: 0.7506     \n",
      "Epoch 87/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.6542 - acc: 0.7548     \n",
      "Epoch 88/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.6496 - acc: 0.7504     \n",
      "Epoch 89/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.6467 - acc: 0.7559     \n",
      "Epoch 90/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.6407 - acc: 0.7566     \n",
      "Epoch 91/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.6158 - acc: 0.7677     \n",
      "Epoch 92/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.6208 - acc: 0.7637     \n",
      "Epoch 93/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.6121 - acc: 0.7697     \n",
      "Epoch 94/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.6212 - acc: 0.7648     \n",
      "Epoch 95/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.5821 - acc: 0.7811     \n",
      "Epoch 96/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.5834 - acc: 0.7794     \n",
      "Epoch 97/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.5637 - acc: 0.7922     \n",
      "Epoch 98/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.5623 - acc: 0.7929     \n",
      "Epoch 99/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.5781 - acc: 0.7785     \n",
      "Epoch 100/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.5753 - acc: 0.7808     \n",
      "Epoch 101/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.5632 - acc: 0.7887     \n",
      "Epoch 102/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.5331 - acc: 0.8011     \n",
      "Epoch 103/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.5319 - acc: 0.8010     \n",
      "Epoch 104/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.5457 - acc: 0.7964     \n",
      "Epoch 105/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.5286 - acc: 0.8044     \n",
      "Epoch 106/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.5302 - acc: 0.8020     \n",
      "Epoch 107/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.5035 - acc: 0.8139     \n",
      "Epoch 108/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.5203 - acc: 0.8056     \n",
      "Epoch 109/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.4677 - acc: 0.8279     \n",
      "Epoch 110/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.4738 - acc: 0.8260     \n",
      "Epoch 111/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.4984 - acc: 0.8083     \n",
      "Epoch 112/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.4641 - acc: 0.8272     \n",
      "Epoch 113/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.4503 - acc: 0.8358     \n",
      "Epoch 114/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.4855 - acc: 0.8141     \n",
      "Epoch 115/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.4745 - acc: 0.8236     \n",
      "Epoch 116/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.4488 - acc: 0.8365     \n",
      "Epoch 117/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.4437 - acc: 0.8353     \n",
      "Epoch 118/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.4237 - acc: 0.8452     \n",
      "Epoch 119/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.3977 - acc: 0.8556     \n",
      "Epoch 120/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.4360 - acc: 0.8381     \n",
      "Epoch 121/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.4106 - acc: 0.8500     \n",
      "Epoch 122/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.3904 - acc: 0.8559     \n",
      "Epoch 123/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.4503 - acc: 0.8341     \n",
      "Epoch 124/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.4151 - acc: 0.8463     \n",
      "Epoch 125/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.3754 - acc: 0.8622     \n",
      "Epoch 126/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.3977 - acc: 0.8516     \n",
      "Epoch 127/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.3710 - acc: 0.8609     \n",
      "Epoch 128/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.3994 - acc: 0.8512     \n",
      "Epoch 129/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.3745 - acc: 0.8617     \n",
      "Epoch 130/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.3803 - acc: 0.8580     \n",
      "Epoch 131/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.3534 - acc: 0.8684     \n",
      "Epoch 132/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.3337 - acc: 0.8806     \n",
      "Epoch 133/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.3319 - acc: 0.8805     \n",
      "Epoch 134/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.3563 - acc: 0.8687     \n",
      "Epoch 135/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.3307 - acc: 0.8804     \n",
      "Epoch 136/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.3496 - acc: 0.8732     \n",
      "Epoch 137/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.3992 - acc: 0.8481     \n",
      "Epoch 138/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.3145 - acc: 0.8841     \n",
      "Epoch 139/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.2950 - acc: 0.8933     \n",
      "Epoch 140/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.3135 - acc: 0.8849     \n",
      "Epoch 141/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.3076 - acc: 0.8894     \n",
      "Epoch 142/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.2811 - acc: 0.8985     \n",
      "Epoch 143/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.3029 - acc: 0.8870     \n",
      "Epoch 144/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.2782 - acc: 0.9010     \n",
      "Epoch 145/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.3286 - acc: 0.8785     \n",
      "Epoch 146/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.3159 - acc: 0.8857     \n",
      "Epoch 147/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.3138 - acc: 0.8834     \n",
      "Epoch 148/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.2609 - acc: 0.9060     \n",
      "Epoch 149/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.2703 - acc: 0.9017     \n",
      "Epoch 150/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.3308 - acc: 0.8791     \n",
      "Epoch 151/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.2651 - acc: 0.9036     \n",
      "Epoch 152/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.2425 - acc: 0.9127     \n",
      "Epoch 153/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.2701 - acc: 0.8988     \n",
      "Epoch 154/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.2666 - acc: 0.9054     \n",
      "Epoch 155/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.2940 - acc: 0.8923     \n",
      "Epoch 156/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.2605 - acc: 0.9084     \n",
      "Epoch 157/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.2538 - acc: 0.9074     \n",
      "Epoch 158/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.2187 - acc: 0.9215     \n",
      "Epoch 159/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.2278 - acc: 0.9186     \n",
      "Epoch 160/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.3202 - acc: 0.8818     \n",
      "Epoch 161/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.2469 - acc: 0.9100     \n",
      "Epoch 162/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.2207 - acc: 0.9217     \n",
      "Epoch 163/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.2318 - acc: 0.9154     \n",
      "Epoch 164/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.1969 - acc: 0.9302     \n",
      "Epoch 165/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.2348 - acc: 0.9130     \n",
      "Epoch 166/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.2444 - acc: 0.9120     \n",
      "Epoch 167/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.1825 - acc: 0.9362     \n",
      "Epoch 168/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.2038 - acc: 0.9310     \n",
      "Epoch 169/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.2063 - acc: 0.9285     \n",
      "Epoch 170/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.1837 - acc: 0.9361     \n",
      "Epoch 171/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.2007 - acc: 0.9270     \n",
      "Epoch 172/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.2128 - acc: 0.9251     \n",
      "Epoch 173/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.2757 - acc: 0.8986     \n",
      "Epoch 174/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.1765 - acc: 0.9378     \n",
      "Epoch 175/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.1703 - acc: 0.9392     \n",
      "Epoch 176/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.2035 - acc: 0.9303     \n",
      "Epoch 177/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.3944 - acc: 0.8687     \n",
      "Epoch 178/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.1661 - acc: 0.9462     \n",
      "Epoch 179/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.1453 - acc: 0.9518     \n",
      "Epoch 180/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.1396 - acc: 0.9550     \n",
      "Epoch 181/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.1327 - acc: 0.9561     \n",
      "Epoch 182/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.1778 - acc: 0.9370     \n",
      "Epoch 183/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.1710 - acc: 0.9398     \n",
      "Epoch 184/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.1940 - acc: 0.9301     \n",
      "Epoch 185/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.1401 - acc: 0.9519     \n",
      "Epoch 186/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.1506 - acc: 0.9478     \n",
      "Epoch 187/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.1601 - acc: 0.9460     \n",
      "Epoch 188/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.2642 - acc: 0.9032     \n",
      "Epoch 189/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.1348 - acc: 0.9552     \n",
      "Epoch 190/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.1624 - acc: 0.9448     \n",
      "Epoch 191/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.1588 - acc: 0.9443     \n",
      "Epoch 192/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.1726 - acc: 0.9379     \n",
      "Epoch 193/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.1606 - acc: 0.9429     \n",
      "Epoch 194/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.1795 - acc: 0.9337     \n",
      "Epoch 195/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.1811 - acc: 0.9323     \n",
      "Epoch 196/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.1883 - acc: 0.9326     \n",
      "Epoch 197/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.1310 - acc: 0.9549     \n",
      "Epoch 198/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.1048 - acc: 0.9665     \n",
      "Epoch 199/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.0940 - acc: 0.9704     \n",
      "Epoch 200/200\n",
      "10314/10314 [==============================] - 0s - loss: 0.1269 - acc: 0.9575     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x13dbdc650>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(300, input_dim = 300, init='normal', activation='relu'))\n",
    "model.add(Dense(100, init='normal', activation='relu'))\n",
    "model.add(Dense(100, init='normal', activation='relu'))\n",
    "model.add(Dense(61, init='normal', activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(train_x, train_y, batch_size = 64, nb_epoch = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "10314/10314 [==============================] - 2s - loss: 3.5811 - acc: 0.2116     \n",
      "Epoch 2/200\n",
      "10314/10314 [==============================] - 1s - loss: 2.8682 - acc: 0.2145     \n",
      "Epoch 3/200\n",
      "10314/10314 [==============================] - 1s - loss: 2.7344 - acc: 0.2152     \n",
      "Epoch 4/200\n",
      "10314/10314 [==============================] - 1s - loss: 2.6274 - acc: 0.2153     \n",
      "Epoch 5/200\n",
      "10314/10314 [==============================] - 1s - loss: 2.5562 - acc: 0.2150     \n",
      "Epoch 6/200\n",
      "10314/10314 [==============================] - 1s - loss: 2.5112 - acc: 0.2181     \n",
      "Epoch 7/200\n",
      "10314/10314 [==============================] - 1s - loss: 2.4773 - acc: 0.2172     \n",
      "Epoch 8/200\n",
      "10314/10314 [==============================] - 1s - loss: 2.4470 - acc: 0.2170     \n",
      "Epoch 9/200\n",
      "10314/10314 [==============================] - 1s - loss: 2.4170 - acc: 0.2211     \n",
      "Epoch 10/200\n",
      "10314/10314 [==============================] - 1s - loss: 2.3848 - acc: 0.2354     \n",
      "Epoch 11/200\n",
      "10314/10314 [==============================] - 1s - loss: 2.3497 - acc: 0.2560     \n",
      "Epoch 12/200\n",
      "10314/10314 [==============================] - 1s - loss: 2.3068 - acc: 0.2902     \n",
      "Epoch 13/200\n",
      "10314/10314 [==============================] - 1s - loss: 2.2554 - acc: 0.3086     \n",
      "Epoch 14/200\n",
      "10314/10314 [==============================] - 1s - loss: 2.2004 - acc: 0.3137     \n",
      "Epoch 15/200\n",
      "10314/10314 [==============================] - 1s - loss: 2.1510 - acc: 0.3163     \n",
      "Epoch 16/200\n",
      "10314/10314 [==============================] - 1s - loss: 2.1109 - acc: 0.3256     \n",
      "Epoch 17/200\n",
      "10314/10314 [==============================] - 1s - loss: 2.0775 - acc: 0.3269     \n",
      "Epoch 18/200\n",
      "10314/10314 [==============================] - 1s - loss: 2.0508 - acc: 0.3347     \n",
      "Epoch 19/200\n",
      "10314/10314 [==============================] - 1s - loss: 2.0276 - acc: 0.3427     \n",
      "Epoch 20/200\n",
      "10314/10314 [==============================] - 1s - loss: 2.0085 - acc: 0.3446     \n",
      "Epoch 21/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.9904 - acc: 0.3479     \n",
      "Epoch 22/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.9756 - acc: 0.3496     \n",
      "Epoch 23/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.9605 - acc: 0.3535     \n",
      "Epoch 24/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.9474 - acc: 0.3568     \n",
      "Epoch 25/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.9355 - acc: 0.3653     \n",
      "Epoch 26/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.9236 - acc: 0.3603     \n",
      "Epoch 27/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.9104 - acc: 0.3688     \n",
      "Epoch 28/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.9005 - acc: 0.3737     \n",
      "Epoch 29/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.8897 - acc: 0.3751     \n",
      "Epoch 30/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.8791 - acc: 0.3785     \n",
      "Epoch 31/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.8679 - acc: 0.3831     \n",
      "Epoch 32/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.8579 - acc: 0.3856     \n",
      "Epoch 33/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.8482 - acc: 0.3901     \n",
      "Epoch 34/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.8409 - acc: 0.3895     \n",
      "Epoch 35/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.8289 - acc: 0.3941     \n",
      "Epoch 36/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.8211 - acc: 0.3951     \n",
      "Epoch 37/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.8138 - acc: 0.3924     \n",
      "Epoch 38/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.8061 - acc: 0.3980     \n",
      "Epoch 39/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.7981 - acc: 0.4019     \n",
      "Epoch 40/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.7894 - acc: 0.4045     \n",
      "Epoch 41/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.7827 - acc: 0.4059     \n",
      "Epoch 42/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.7731 - acc: 0.4058     \n",
      "Epoch 43/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.7679 - acc: 0.4076     \n",
      "Epoch 44/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.7621 - acc: 0.4079     \n",
      "Epoch 45/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.7571 - acc: 0.4075     \n",
      "Epoch 46/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.7512 - acc: 0.4107     \n",
      "Epoch 47/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.7420 - acc: 0.4124     \n",
      "Epoch 48/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.7425 - acc: 0.4148     \n",
      "Epoch 49/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.7323 - acc: 0.4172     \n",
      "Epoch 50/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.7277 - acc: 0.4103     \n",
      "Epoch 51/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.7201 - acc: 0.4173     \n",
      "Epoch 52/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.7181 - acc: 0.4142     \n",
      "Epoch 53/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.7141 - acc: 0.4161     \n",
      "Epoch 54/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.7097 - acc: 0.4168     \n",
      "Epoch 55/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.7036 - acc: 0.4194     \n",
      "Epoch 56/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.6982 - acc: 0.4256     \n",
      "Epoch 57/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.6965 - acc: 0.4233     \n",
      "Epoch 58/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.6915 - acc: 0.4237     \n",
      "Epoch 59/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.6858 - acc: 0.4232     \n",
      "Epoch 60/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.6831 - acc: 0.4270     \n",
      "Epoch 61/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.6770 - acc: 0.4288     \n",
      "Epoch 62/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.6764 - acc: 0.4227     \n",
      "Epoch 63/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.6691 - acc: 0.4249     \n",
      "Epoch 64/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.6685 - acc: 0.4308     \n",
      "Epoch 65/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.6618 - acc: 0.4313     \n",
      "Epoch 66/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.6579 - acc: 0.4335     \n",
      "Epoch 67/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.6558 - acc: 0.4305     \n",
      "Epoch 68/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.6517 - acc: 0.4323     \n",
      "Epoch 69/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.6442 - acc: 0.4347     \n",
      "Epoch 70/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.6431 - acc: 0.4380     \n",
      "Epoch 71/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.6442 - acc: 0.4342     \n",
      "Epoch 72/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.6371 - acc: 0.4367     \n",
      "Epoch 73/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.6381 - acc: 0.4362     \n",
      "Epoch 74/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.6262 - acc: 0.4398     \n",
      "Epoch 75/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.6321 - acc: 0.4346     \n",
      "Epoch 76/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.6260 - acc: 0.4448     \n",
      "Epoch 77/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.6187 - acc: 0.4414     \n",
      "Epoch 78/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.6170 - acc: 0.4412     \n",
      "Epoch 79/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.6166 - acc: 0.4467     \n",
      "Epoch 80/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.6066 - acc: 0.4513     \n",
      "Epoch 81/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.6110 - acc: 0.4495     \n",
      "Epoch 82/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.6080 - acc: 0.4454     \n",
      "Epoch 83/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.5981 - acc: 0.4519     \n",
      "Epoch 84/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.5938 - acc: 0.4533     \n",
      "Epoch 85/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.5985 - acc: 0.4519     \n",
      "Epoch 86/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.5944 - acc: 0.4562     \n",
      "Epoch 87/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.5844 - acc: 0.4606     \n",
      "Epoch 88/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.5887 - acc: 0.4605     \n",
      "Epoch 89/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.5879 - acc: 0.4606     \n",
      "Epoch 90/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.5843 - acc: 0.4600     \n",
      "Epoch 91/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.5779 - acc: 0.4592     \n",
      "Epoch 92/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.5690 - acc: 0.4689     \n",
      "Epoch 93/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.5696 - acc: 0.4657     \n",
      "Epoch 94/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.5660 - acc: 0.4668     \n",
      "Epoch 95/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.5692 - acc: 0.4649     \n",
      "Epoch 96/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.5586 - acc: 0.4661     \n",
      "Epoch 97/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.5599 - acc: 0.4668     \n",
      "Epoch 98/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.5611 - acc: 0.4660     \n",
      "Epoch 99/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.5556 - acc: 0.4676     \n",
      "Epoch 100/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.5500 - acc: 0.4723     \n",
      "Epoch 101/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.5505 - acc: 0.4702     \n",
      "Epoch 102/200\n",
      "10314/10314 [==============================] - 2s - loss: 1.5504 - acc: 0.4739     \n",
      "Epoch 103/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.5429 - acc: 0.4732     \n",
      "Epoch 104/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.5409 - acc: 0.4699     \n",
      "Epoch 105/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.5380 - acc: 0.4704     \n",
      "Epoch 106/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.5378 - acc: 0.4769     \n",
      "Epoch 107/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.5311 - acc: 0.4767     \n",
      "Epoch 108/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.5335 - acc: 0.4764     \n",
      "Epoch 109/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.5245 - acc: 0.4768     \n",
      "Epoch 110/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.5238 - acc: 0.4784     \n",
      "Epoch 111/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.5189 - acc: 0.4789     \n",
      "Epoch 112/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.5252 - acc: 0.4789     \n",
      "Epoch 113/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.5118 - acc: 0.4857     \n",
      "Epoch 114/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.5123 - acc: 0.4821     \n",
      "Epoch 115/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.5142 - acc: 0.4806     \n",
      "Epoch 116/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.5049 - acc: 0.4811     \n",
      "Epoch 117/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.5040 - acc: 0.4813     \n",
      "Epoch 118/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.5093 - acc: 0.4778     \n",
      "Epoch 119/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.5005 - acc: 0.4841     \n",
      "Epoch 120/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.4956 - acc: 0.4835     \n",
      "Epoch 121/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.4979 - acc: 0.4841     \n",
      "Epoch 122/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.4943 - acc: 0.4857     \n",
      "Epoch 123/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.4868 - acc: 0.4893     \n",
      "Epoch 124/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.4895 - acc: 0.4849     \n",
      "Epoch 125/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.4876 - acc: 0.4905     \n",
      "Epoch 126/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.4846 - acc: 0.4907     \n",
      "Epoch 127/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.4806 - acc: 0.4898     \n",
      "Epoch 128/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.4832 - acc: 0.4848     \n",
      "Epoch 129/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.4736 - acc: 0.4902     \n",
      "Epoch 130/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.4695 - acc: 0.4989     \n",
      "Epoch 131/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.4804 - acc: 0.4910     \n",
      "Epoch 132/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.4730 - acc: 0.4949     \n",
      "Epoch 133/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.4651 - acc: 0.4967     \n",
      "Epoch 134/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.4687 - acc: 0.4934     \n",
      "Epoch 135/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.4650 - acc: 0.4944     \n",
      "Epoch 136/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.4629 - acc: 0.4945     \n",
      "Epoch 137/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.4578 - acc: 0.4979     \n",
      "Epoch 138/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.4567 - acc: 0.4979     \n",
      "Epoch 139/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.4583 - acc: 0.5002     \n",
      "Epoch 140/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.4529 - acc: 0.4978     \n",
      "Epoch 141/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.4479 - acc: 0.5036     \n",
      "Epoch 142/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.4454 - acc: 0.5048     \n",
      "Epoch 143/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.4481 - acc: 0.5015     \n",
      "Epoch 144/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.4410 - acc: 0.5056     \n",
      "Epoch 145/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.4433 - acc: 0.5066     \n",
      "Epoch 146/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.4378 - acc: 0.5078     \n",
      "Epoch 147/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.4354 - acc: 0.5081     \n",
      "Epoch 148/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.4308 - acc: 0.5096     \n",
      "Epoch 149/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.4368 - acc: 0.5076     \n",
      "Epoch 150/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.4250 - acc: 0.5111     \n",
      "Epoch 151/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.4327 - acc: 0.5099     \n",
      "Epoch 152/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.4258 - acc: 0.5093     \n",
      "Epoch 153/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.4252 - acc: 0.5086     \n",
      "Epoch 154/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.4304 - acc: 0.5068     \n",
      "Epoch 155/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.4244 - acc: 0.5096     \n",
      "Epoch 156/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.4245 - acc: 0.5083     \n",
      "Epoch 157/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.4211 - acc: 0.5119     \n",
      "Epoch 158/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.4115 - acc: 0.5179     \n",
      "Epoch 159/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.4155 - acc: 0.5131     \n",
      "Epoch 160/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.4147 - acc: 0.5133     \n",
      "Epoch 161/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.4078 - acc: 0.5211     \n",
      "Epoch 162/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.4060 - acc: 0.5133     \n",
      "Epoch 163/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.4076 - acc: 0.5112     \n",
      "Epoch 164/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.4030 - acc: 0.5163     \n",
      "Epoch 165/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.4026 - acc: 0.5171     \n",
      "Epoch 166/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.4051 - acc: 0.5119     \n",
      "Epoch 167/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.3970 - acc: 0.5197     \n",
      "Epoch 168/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.3945 - acc: 0.5210     \n",
      "Epoch 169/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.3950 - acc: 0.5148     \n",
      "Epoch 170/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.3901 - acc: 0.5181     \n",
      "Epoch 171/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.4013 - acc: 0.5108     \n",
      "Epoch 172/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.3849 - acc: 0.5292     \n",
      "Epoch 173/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.3894 - acc: 0.5219     \n",
      "Epoch 174/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.3892 - acc: 0.5241     \n",
      "Epoch 175/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.3894 - acc: 0.5230     \n",
      "Epoch 176/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.3813 - acc: 0.5289     \n",
      "Epoch 177/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.3837 - acc: 0.5215     \n",
      "Epoch 178/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.3828 - acc: 0.5200     \n",
      "Epoch 179/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.3822 - acc: 0.5250     \n",
      "Epoch 180/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.3810 - acc: 0.5213     \n",
      "Epoch 181/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.3723 - acc: 0.5277     \n",
      "Epoch 182/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.3808 - acc: 0.5271     \n",
      "Epoch 183/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.3695 - acc: 0.5283     \n",
      "Epoch 184/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.3703 - acc: 0.5283     \n",
      "Epoch 185/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.3672 - acc: 0.5284     \n",
      "Epoch 186/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.3637 - acc: 0.5349     \n",
      "Epoch 187/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.3698 - acc: 0.5242     \n",
      "Epoch 188/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.3678 - acc: 0.5282     \n",
      "Epoch 189/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.3663 - acc: 0.5295     \n",
      "Epoch 190/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.3662 - acc: 0.5279     \n",
      "Epoch 191/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.3592 - acc: 0.5329     \n",
      "Epoch 192/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.3603 - acc: 0.5344     \n",
      "Epoch 193/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.3577 - acc: 0.5384     \n",
      "Epoch 194/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.3591 - acc: 0.5293     \n",
      "Epoch 195/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.3532 - acc: 0.5352     \n",
      "Epoch 196/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.3553 - acc: 0.5302     \n",
      "Epoch 197/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.3490 - acc: 0.5397     \n",
      "Epoch 198/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.3444 - acc: 0.5386     \n",
      "Epoch 199/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.3517 - acc: 0.5349     \n",
      "Epoch 200/200\n",
      "10314/10314 [==============================] - 1s - loss: 1.3578 - acc: 0.5319     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x13eeefdd0>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(300, input_dim=300, init='normal', activation='relu',\\\n",
    "          W_regularizer=l2(0.0001), activity_regularizer=activity_l2(0.0001)))\n",
    "model.add(Dense(150, init='normal', activation='relu',\\\n",
    "          W_regularizer=l2(0.0001), activity_regularizer=activity_l2(0.0001)))\n",
    "model.add(Dense(150, init='normal', activation='relu',\\\n",
    "          W_regularizer=l2(0.0001), activity_regularizer=activity_l2(0.0001)))\n",
    "\n",
    "model.add(Dense(61, init='normal', activation='softmax'))\n",
    "sgd = SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "model.fit(train_x, train_y, batch_size = 32, nb_epoch = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1216/1286 [===========================>..] - ETA: 0s"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>essay_set</th>\n",
       "      <th>essay</th>\n",
       "      <th>rater1_domain1</th>\n",
       "      <th>rater2_domain1</th>\n",
       "      <th>domain1_score</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1204</th>\n",
       "      <td>20716</td>\n",
       "      <td>8</td>\n",
       "      <td>A long time ago when I was in third grade I h...</td>\n",
       "      <td>18</td>\n",
       "      <td>16</td>\n",
       "      <td>34</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1205</th>\n",
       "      <td>20717</td>\n",
       "      <td>8</td>\n",
       "      <td>Softball has to be one of the single most gre...</td>\n",
       "      <td>21</td>\n",
       "      <td>26</td>\n",
       "      <td>46</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1206</th>\n",
       "      <td>20727</td>\n",
       "      <td>8</td>\n",
       "      <td>Laugher Laughter is to express delight, fun, ...</td>\n",
       "      <td>16</td>\n",
       "      <td>15</td>\n",
       "      <td>31</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1207</th>\n",
       "      <td>20735</td>\n",
       "      <td>8</td>\n",
       "      <td>one time when i was skateboarding with my fri...</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>20</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1208</th>\n",
       "      <td>20800</td>\n",
       "      <td>8</td>\n",
       "      <td>This true story might sound some what cheesy,...</td>\n",
       "      <td>18</td>\n",
       "      <td>15</td>\n",
       "      <td>33</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1209</th>\n",
       "      <td>20804</td>\n",
       "      <td>8</td>\n",
       "      <td>Laughter is important in my life for many reas...</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1210</th>\n",
       "      <td>20819</td>\n",
       "      <td>8</td>\n",
       "      <td>LaughterI laugh everyday. I feel a lot of dif...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>30</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1211</th>\n",
       "      <td>20825</td>\n",
       "      <td>8</td>\n",
       "      <td>In this essay I will be telling you about why...</td>\n",
       "      <td>15</td>\n",
       "      <td>18</td>\n",
       "      <td>33</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1212</th>\n",
       "      <td>20832</td>\n",
       "      <td>8</td>\n",
       "      <td>Laughter is something that is there when you ...</td>\n",
       "      <td>20</td>\n",
       "      <td>18</td>\n",
       "      <td>36</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1213</th>\n",
       "      <td>20841</td>\n",
       "      <td>8</td>\n",
       "      <td>As we understand the benefits of laughter, we...</td>\n",
       "      <td>17</td>\n",
       "      <td>16</td>\n",
       "      <td>33</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1214</th>\n",
       "      <td>20853</td>\n",
       "      <td>8</td>\n",
       "      <td>DAIRY @CAPS1 boyfriend and I spend a lot of ti...</td>\n",
       "      <td>15</td>\n",
       "      <td>17</td>\n",
       "      <td>32</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1215</th>\n",
       "      <td>20861</td>\n",
       "      <td>8</td>\n",
       "      <td>Laughter Laughter has alw...</td>\n",
       "      <td>19</td>\n",
       "      <td>20</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1216</th>\n",
       "      <td>20862</td>\n",
       "      <td>8</td>\n",
       "      <td>\"@CAPS1!\" @CAPS2 society today loves laughter...</td>\n",
       "      <td>20</td>\n",
       "      <td>25</td>\n",
       "      <td>45</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1217</th>\n",
       "      <td>20869</td>\n",
       "      <td>8</td>\n",
       "      <td>A day ...</td>\n",
       "      <td>20</td>\n",
       "      <td>16</td>\n",
       "      <td>36</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1218</th>\n",
       "      <td>20877</td>\n",
       "      <td>8</td>\n",
       "      <td>I have had many great times with my grandpa. ...</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1219</th>\n",
       "      <td>20881</td>\n",
       "      <td>8</td>\n",
       "      <td>Once upon a time, on a late rainy night my fri...</td>\n",
       "      <td>16</td>\n",
       "      <td>12</td>\n",
       "      <td>28</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1220</th>\n",
       "      <td>20919</td>\n",
       "      <td>8</td>\n",
       "      <td>Laughter is a sign of good relationships. To ...</td>\n",
       "      <td>15</td>\n",
       "      <td>19</td>\n",
       "      <td>34</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1221</th>\n",
       "      <td>20948</td>\n",
       "      <td>8</td>\n",
       "      <td>During the @CAPS1 break my family went to @LO...</td>\n",
       "      <td>19</td>\n",
       "      <td>20</td>\n",
       "      <td>40</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1222</th>\n",
       "      <td>20987</td>\n",
       "      <td>8</td>\n",
       "      <td>While you being to make any kinda of relatio...</td>\n",
       "      <td>20</td>\n",
       "      <td>15</td>\n",
       "      <td>35</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1223</th>\n",
       "      <td>20993</td>\n",
       "      <td>8</td>\n",
       "      <td>Many have heard that laughing makes you live ...</td>\n",
       "      <td>20</td>\n",
       "      <td>25</td>\n",
       "      <td>45</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1224</th>\n",
       "      <td>21004</td>\n",
       "      <td>8</td>\n",
       "      <td>Laughter can sometimes be something that every...</td>\n",
       "      <td>14</td>\n",
       "      <td>15</td>\n",
       "      <td>29</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1225</th>\n",
       "      <td>21053</td>\n",
       "      <td>8</td>\n",
       "      <td>I live in a life full of laughter, I can not t...</td>\n",
       "      <td>13</td>\n",
       "      <td>20</td>\n",
       "      <td>30</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1226</th>\n",
       "      <td>21055</td>\n",
       "      <td>8</td>\n",
       "      <td>Laughter is a huge part oh building friendshi...</td>\n",
       "      <td>15</td>\n",
       "      <td>20</td>\n",
       "      <td>35</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1227</th>\n",
       "      <td>21060</td>\n",
       "      <td>8</td>\n",
       "      <td>The human brain is a piece of work. Its the k...</td>\n",
       "      <td>20</td>\n",
       "      <td>25</td>\n",
       "      <td>45</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1228</th>\n",
       "      <td>21067</td>\n",
       "      <td>8</td>\n",
       "      <td>It all started with a book @DATE1 @DATE1 ago....</td>\n",
       "      <td>20</td>\n",
       "      <td>21</td>\n",
       "      <td>41</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1229</th>\n",
       "      <td>21083</td>\n",
       "      <td>8</td>\n",
       "      <td>The members of my immediate family are known ...</td>\n",
       "      <td>20</td>\n",
       "      <td>21</td>\n",
       "      <td>41</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1230</th>\n",
       "      <td>21111</td>\n",
       "      <td>8</td>\n",
       "      <td>Everyone knows that when it comes to coach @P...</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>36</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1231</th>\n",
       "      <td>21122</td>\n",
       "      <td>8</td>\n",
       "      <td>The wind rose and fell, whipping against the ...</td>\n",
       "      <td>25</td>\n",
       "      <td>21</td>\n",
       "      <td>46</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1232</th>\n",
       "      <td>21133</td>\n",
       "      <td>8</td>\n",
       "      <td>My @CAPS1  @CAPS2 was a...</td>\n",
       "      <td>20</td>\n",
       "      <td>15</td>\n",
       "      <td>35</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1233</th>\n",
       "      <td>21147</td>\n",
       "      <td>8</td>\n",
       "      <td>Laughter is a huge part of life, in fact it i...</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>40</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1256</th>\n",
       "      <td>21315</td>\n",
       "      <td>8</td>\n",
       "      <td>Laughter: A @CAPS1 @CAPS2 @CAPS3 of the variet...</td>\n",
       "      <td>20</td>\n",
       "      <td>15</td>\n",
       "      <td>35</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1257</th>\n",
       "      <td>21339</td>\n",
       "      <td>8</td>\n",
       "      <td>@PERSON1                               @CAPS1 ...</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>34</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1258</th>\n",
       "      <td>21345</td>\n",
       "      <td>8</td>\n",
       "      <td>Laughters importance in human life   In life t...</td>\n",
       "      <td>15</td>\n",
       "      <td>17</td>\n",
       "      <td>32</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1259</th>\n",
       "      <td>21348</td>\n",
       "      <td>8</td>\n",
       "      <td>In all of our lives there is that one person...</td>\n",
       "      <td>20</td>\n",
       "      <td>19</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1260</th>\n",
       "      <td>21352</td>\n",
       "      <td>8</td>\n",
       "      <td>The @CAPS1 of The @CAPS2 @C...</td>\n",
       "      <td>17</td>\n",
       "      <td>20</td>\n",
       "      <td>37</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1261</th>\n",
       "      <td>21358</td>\n",
       "      <td>8</td>\n",
       "      <td>Laughter is a @CAPS1 @CAPS2 do things like mus...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>46</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1262</th>\n",
       "      <td>21366</td>\n",
       "      <td>8</td>\n",
       "      <td>I slowly got up and watched as paint dripped ...</td>\n",
       "      <td>20</td>\n",
       "      <td>21</td>\n",
       "      <td>41</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1263</th>\n",
       "      <td>21369</td>\n",
       "      <td>8</td>\n",
       "      <td>Laughter is the key I think that being happy ...</td>\n",
       "      <td>15</td>\n",
       "      <td>14</td>\n",
       "      <td>29</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1264</th>\n",
       "      <td>21370</td>\n",
       "      <td>8</td>\n",
       "      <td>My smile is a mask. It disguises all the past...</td>\n",
       "      <td>16</td>\n",
       "      <td>15</td>\n",
       "      <td>31</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1265</th>\n",
       "      <td>21373</td>\n",
       "      <td>8</td>\n",
       "      <td>In the ninth grade I was required to give a s...</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1266</th>\n",
       "      <td>21388</td>\n",
       "      <td>8</td>\n",
       "      <td>Family trip @CAPS1 the @DATE1 my famil...</td>\n",
       "      <td>20</td>\n",
       "      <td>18</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1267</th>\n",
       "      <td>21419</td>\n",
       "      <td>8</td>\n",
       "      <td>The @CAPS1 of Laughter   The @CAPS1 of a singl...</td>\n",
       "      <td>20</td>\n",
       "      <td>23</td>\n",
       "      <td>43</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1268</th>\n",
       "      <td>21426</td>\n",
       "      <td>8</td>\n",
       "      <td>When I was adopted, I didn't know what to exp...</td>\n",
       "      <td>21</td>\n",
       "      <td>20</td>\n",
       "      <td>41</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1269</th>\n",
       "      <td>21449</td>\n",
       "      <td>8</td>\n",
       "      <td>One day my friend and I were waking up late in...</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1270</th>\n",
       "      <td>21480</td>\n",
       "      <td>8</td>\n",
       "      <td>Laughter in life is a good thing to have I b...</td>\n",
       "      <td>15</td>\n",
       "      <td>17</td>\n",
       "      <td>32</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1271</th>\n",
       "      <td>21490</td>\n",
       "      <td>8</td>\n",
       "      <td>Laughter saved my life. Growing up, my life ...</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>50</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1272</th>\n",
       "      <td>21495</td>\n",
       "      <td>8</td>\n",
       "      <td>Every one laughs, i love to laugh in fact...</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>36</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1273</th>\n",
       "      <td>21514</td>\n",
       "      <td>8</td>\n",
       "      <td>I think laughter should be a huge part in eve...</td>\n",
       "      <td>15</td>\n",
       "      <td>19</td>\n",
       "      <td>34</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1274</th>\n",
       "      <td>21522</td>\n",
       "      <td>8</td>\n",
       "      <td>It was the middle of @DATE1 and I was ten, bo...</td>\n",
       "      <td>20</td>\n",
       "      <td>18</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1275</th>\n",
       "      <td>21525</td>\n",
       "      <td>8</td>\n",
       "      <td>Laughter can be a good think and sometimes it...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>30</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1276</th>\n",
       "      <td>21530</td>\n",
       "      <td>8</td>\n",
       "      <td>Laughter, one of the most joyous things there...</td>\n",
       "      <td>25</td>\n",
       "      <td>21</td>\n",
       "      <td>46</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1277</th>\n",
       "      <td>21533</td>\n",
       "      <td>8</td>\n",
       "      <td>My dad and i went out to teach me how to driv...</td>\n",
       "      <td>17</td>\n",
       "      <td>18</td>\n",
       "      <td>35</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1278</th>\n",
       "      <td>21537</td>\n",
       "      <td>8</td>\n",
       "      <td>In the @DATE1 of @NUM1' I spent two weeks at ...</td>\n",
       "      <td>16</td>\n",
       "      <td>17</td>\n",
       "      <td>33</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1279</th>\n",
       "      <td>21548</td>\n",
       "      <td>8</td>\n",
       "      <td>Laughter, one of the wonderful things that we...</td>\n",
       "      <td>15</td>\n",
       "      <td>20</td>\n",
       "      <td>35</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1280</th>\n",
       "      <td>21555</td>\n",
       "      <td>8</td>\n",
       "      <td>The elements of laughter is to show emotion t...</td>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "      <td>25</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1281</th>\n",
       "      <td>21558</td>\n",
       "      <td>8</td>\n",
       "      <td>Many believe that laughter is the key to the ...</td>\n",
       "      <td>18</td>\n",
       "      <td>20</td>\n",
       "      <td>38</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1282</th>\n",
       "      <td>21563</td>\n",
       "      <td>8</td>\n",
       "      <td>The sound of laughter could be one that break...</td>\n",
       "      <td>21</td>\n",
       "      <td>25</td>\n",
       "      <td>46</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1283</th>\n",
       "      <td>21570</td>\n",
       "      <td>8</td>\n",
       "      <td>Laughter A true story that involves @CAPS4 and...</td>\n",
       "      <td>16</td>\n",
       "      <td>15</td>\n",
       "      <td>31</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1284</th>\n",
       "      <td>21609</td>\n",
       "      <td>8</td>\n",
       "      <td>Laughter, to me, is an important aspect of my...</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>40</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1285</th>\n",
       "      <td>21615</td>\n",
       "      <td>8</td>\n",
       "      <td>@ORGANIZATION1  ...</td>\n",
       "      <td>15</td>\n",
       "      <td>17</td>\n",
       "      <td>32</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>82 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      essay_id  essay_set                                              essay  \\\n",
       "1204     20716          8   A long time ago when I was in third grade I h...   \n",
       "1205     20717          8   Softball has to be one of the single most gre...   \n",
       "1206     20727          8   Laugher Laughter is to express delight, fun, ...   \n",
       "1207     20735          8   one time when i was skateboarding with my fri...   \n",
       "1208     20800          8   This true story might sound some what cheesy,...   \n",
       "1209     20804          8  Laughter is important in my life for many reas...   \n",
       "1210     20819          8   LaughterI laugh everyday. I feel a lot of dif...   \n",
       "1211     20825          8   In this essay I will be telling you about why...   \n",
       "1212     20832          8   Laughter is something that is there when you ...   \n",
       "1213     20841          8   As we understand the benefits of laughter, we...   \n",
       "1214     20853          8  DAIRY @CAPS1 boyfriend and I spend a lot of ti...   \n",
       "1215     20861          8                       Laughter Laughter has alw...   \n",
       "1216     20862          8   \"@CAPS1!\" @CAPS2 society today loves laughter...   \n",
       "1217     20869          8                                          A day ...   \n",
       "1218     20877          8   I have had many great times with my grandpa. ...   \n",
       "1219     20881          8  Once upon a time, on a late rainy night my fri...   \n",
       "1220     20919          8   Laughter is a sign of good relationships. To ...   \n",
       "1221     20948          8   During the @CAPS1 break my family went to @LO...   \n",
       "1222     20987          8    While you being to make any kinda of relatio...   \n",
       "1223     20993          8   Many have heard that laughing makes you live ...   \n",
       "1224     21004          8  Laughter can sometimes be something that every...   \n",
       "1225     21053          8  I live in a life full of laughter, I can not t...   \n",
       "1226     21055          8   Laughter is a huge part oh building friendshi...   \n",
       "1227     21060          8   The human brain is a piece of work. Its the k...   \n",
       "1228     21067          8   It all started with a book @DATE1 @DATE1 ago....   \n",
       "1229     21083          8   The members of my immediate family are known ...   \n",
       "1230     21111          8   Everyone knows that when it comes to coach @P...   \n",
       "1231     21122          8   The wind rose and fell, whipping against the ...   \n",
       "1232     21133          8                         My @CAPS1  @CAPS2 was a...   \n",
       "1233     21147          8   Laughter is a huge part of life, in fact it i...   \n",
       "...        ...        ...                                                ...   \n",
       "1256     21315          8  Laughter: A @CAPS1 @CAPS2 @CAPS3 of the variet...   \n",
       "1257     21339          8  @PERSON1                               @CAPS1 ...   \n",
       "1258     21345          8  Laughters importance in human life   In life t...   \n",
       "1259     21348          8    In all of our lives there is that one person...   \n",
       "1260     21352          8                     The @CAPS1 of The @CAPS2 @C...   \n",
       "1261     21358          8  Laughter is a @CAPS1 @CAPS2 do things like mus...   \n",
       "1262     21366          8   I slowly got up and watched as paint dripped ...   \n",
       "1263     21369          8   Laughter is the key I think that being happy ...   \n",
       "1264     21370          8   My smile is a mask. It disguises all the past...   \n",
       "1265     21373          8   In the ninth grade I was required to give a s...   \n",
       "1266     21388          8          Family trip @CAPS1 the @DATE1 my famil...   \n",
       "1267     21419          8  The @CAPS1 of Laughter   The @CAPS1 of a singl...   \n",
       "1268     21426          8   When I was adopted, I didn't know what to exp...   \n",
       "1269     21449          8  One day my friend and I were waking up late in...   \n",
       "1270     21480          8    Laughter in life is a good thing to have I b...   \n",
       "1271     21490          8    Laughter saved my life. Growing up, my life ...   \n",
       "1272     21495          8       Every one laughs, i love to laugh in fact...   \n",
       "1273     21514          8   I think laughter should be a huge part in eve...   \n",
       "1274     21522          8   It was the middle of @DATE1 and I was ten, bo...   \n",
       "1275     21525          8   Laughter can be a good think and sometimes it...   \n",
       "1276     21530          8   Laughter, one of the most joyous things there...   \n",
       "1277     21533          8   My dad and i went out to teach me how to driv...   \n",
       "1278     21537          8   In the @DATE1 of @NUM1' I spent two weeks at ...   \n",
       "1279     21548          8   Laughter, one of the wonderful things that we...   \n",
       "1280     21555          8   The elements of laughter is to show emotion t...   \n",
       "1281     21558          8   Many believe that laughter is the key to the ...   \n",
       "1282     21563          8   The sound of laughter could be one that break...   \n",
       "1283     21570          8  Laughter A true story that involves @CAPS4 and...   \n",
       "1284     21609          8   Laughter, to me, is an important aspect of my...   \n",
       "1285     21615          8                                @ORGANIZATION1  ...   \n",
       "\n",
       "      rater1_domain1  rater2_domain1  domain1_score  prediction  \n",
       "1204              18              16             34          40  \n",
       "1205              21              26             46          40  \n",
       "1206              16              15             31          40  \n",
       "1207              10              10             20          40  \n",
       "1208              18              15             33          40  \n",
       "1209              20              20             40          40  \n",
       "1210              15              15             30          40  \n",
       "1211              15              18             33           3  \n",
       "1212              20              18             36          40  \n",
       "1213              17              16             33          40  \n",
       "1214              15              17             32          40  \n",
       "1215              19              20             40          40  \n",
       "1216              20              25             45          40  \n",
       "1217              20              16             36          40  \n",
       "1218              20              20             40          40  \n",
       "1219              16              12             28          40  \n",
       "1220              15              19             34          40  \n",
       "1221              19              20             40          16  \n",
       "1222              20              15             35          40  \n",
       "1223              20              25             45          40  \n",
       "1224              14              15             29          40  \n",
       "1225              13              20             30          40  \n",
       "1226              15              20             35          40  \n",
       "1227              20              25             45          40  \n",
       "1228              20              21             41          40  \n",
       "1229              20              21             41          40  \n",
       "1230              18              18             36          40  \n",
       "1231              25              21             46           3  \n",
       "1232              20              15             35          40  \n",
       "1233              20              20             40          24  \n",
       "...              ...             ...            ...         ...  \n",
       "1256              20              15             35          40  \n",
       "1257              17              17             34          40  \n",
       "1258              15              17             32          40  \n",
       "1259              20              19             40          40  \n",
       "1260              17              20             37          40  \n",
       "1261              23              23             46          40  \n",
       "1262              20              21             41          40  \n",
       "1263              15              14             29          40  \n",
       "1264              16              15             31          40  \n",
       "1265              20              20             40          40  \n",
       "1266              20              18             40          40  \n",
       "1267              20              23             43          40  \n",
       "1268              21              20             41          40  \n",
       "1269              16              16             32          16  \n",
       "1270              15              17             32          40  \n",
       "1271              25              25             50          40  \n",
       "1272              18              18             36          40  \n",
       "1273              15              19             34          40  \n",
       "1274              20              18             40          40  \n",
       "1275              15              15             30          40  \n",
       "1276              25              21             46          40  \n",
       "1277              17              18             35          40  \n",
       "1278              16              17             33          40  \n",
       "1279              15              20             35          40  \n",
       "1280              10              15             25          40  \n",
       "1281              18              20             38          40  \n",
       "1282              21              25             46          40  \n",
       "1283              16              15             31          40  \n",
       "1284              20              20             40           3  \n",
       "1285              15              17             32          24  \n",
       "\n",
       "[82 rows x 7 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_y = model.predict_classes(test_x)\n",
    "test[\"prediction\"] = pred_y\n",
    "test[test[\"essay_set\"] == 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result_y = np.zeros((len(pred_y),3))\n",
    "result_y[:, 0 ] = test[\"essay_set\"].values\n",
    "result_y[:,1] = test[\"essay_id\"].values\n",
    "result_y[:,2] = pred_y\n",
    "result_y = result_y.astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "true_y = np.zeros((len(pred_y),3))\n",
    "true_y[:, 0 ] = test[\"essay_set\"].values\n",
    "true_y[:,1] = test[\"essay_id\"].values\n",
    "true_y[:,2] = labels_test\n",
    "true_y = true_y.astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kappa for each set [ 0.44044698  0.57673174  0.55416614  0.71316647  0.68496903  0.60792848\n",
      "  0.52000519  0.00146303]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.53439040024221085"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate.evaluate(result_y, true_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "one_hot_kappa(train_y, model.predict(train_x), train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kappa for each set [ 0.47483219  0.53685594  0.72348302  0.76873897  0.77758093  0.77304073\n",
      "  0.58950121  0.30666794]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.64320712745851172"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_kappa(train_y, model.predict(train_x), train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tfdif regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "MAX_NB_WORDS = 2500\n",
    "tokenizer = keras.preprocessing.text.Tokenizer(nb_words = MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(texts_train + texts_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28587 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[130]]"
      ]
     },
     "execution_count": 366,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.texts_to_sequences(['hello world combinations'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_x = tokenizer.texts_to_matrix(texts_train, mode = 'tfidf')\n",
    "train_y = labels_train.astype('float32')\n",
    "test_x = tokenizer.texts_to_matrix(texts_test, mode = 'tfidf')\n",
    "test_y = labels_test.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8491, 2500)"
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "8491/8491 [==============================] - 22s - loss: 63.7668 - mean_absolute_error: 2.0933    \n",
      "Epoch 2/50\n",
      "8491/8491 [==============================] - 20s - loss: 9.5773 - mean_absolute_error: 0.6979    \n",
      "Epoch 3/50\n",
      "8491/8491 [==============================] - 19s - loss: 6.2169 - mean_absolute_error: 0.4786    \n",
      "Epoch 4/50\n",
      "8491/8491 [==============================] - 19s - loss: 5.0973 - mean_absolute_error: 0.4334    \n",
      "Epoch 5/50\n",
      "8491/8491 [==============================] - 19s - loss: 4.5388 - mean_absolute_error: 0.4272    \n",
      "Epoch 6/50\n",
      "8491/8491 [==============================] - 19s - loss: 3.9764 - mean_absolute_error: 0.3657    \n",
      "Epoch 7/50\n",
      "8491/8491 [==============================] - 19s - loss: 3.5921 - mean_absolute_error: 0.3204    \n",
      "Epoch 8/50\n",
      "8491/8491 [==============================] - 18s - loss: 3.3834 - mean_absolute_error: 0.3134    \n",
      "Epoch 9/50\n",
      "8491/8491 [==============================] - 18s - loss: 3.2156 - mean_absolute_error: 0.3093    \n",
      "Epoch 10/50\n",
      "8491/8491 [==============================] - 18s - loss: 3.0814 - mean_absolute_error: 0.2948    \n",
      "Epoch 11/50\n",
      "8491/8491 [==============================] - 18s - loss: 2.9656 - mean_absolute_error: 0.2917    \n",
      "Epoch 12/50\n",
      "8491/8491 [==============================] - 18s - loss: 2.8658 - mean_absolute_error: 0.2779    \n",
      "Epoch 13/50\n",
      "8491/8491 [==============================] - 18s - loss: 2.7317 - mean_absolute_error: 0.2422    \n",
      "Epoch 14/50\n",
      "8491/8491 [==============================] - 18s - loss: 2.6603 - mean_absolute_error: 0.2350    \n",
      "Epoch 15/50\n",
      "8491/8491 [==============================] - 18s - loss: 2.6151 - mean_absolute_error: 0.2405    \n",
      "Epoch 16/50\n",
      "8491/8491 [==============================] - 18s - loss: 2.5783 - mean_absolute_error: 0.2469    \n",
      "Epoch 17/50\n",
      "8491/8491 [==============================] - 18s - loss: 2.5145 - mean_absolute_error: 0.2374    \n",
      "Epoch 18/50\n",
      "8491/8491 [==============================] - 18s - loss: 2.4552 - mean_absolute_error: 0.2244    \n",
      "Epoch 19/50\n",
      "8491/8491 [==============================] - 18s - loss: 2.3960 - mean_absolute_error: 0.2152    \n",
      "Epoch 20/50\n",
      "8491/8491 [==============================] - 18s - loss: 2.3518 - mean_absolute_error: 0.2110    \n",
      "Epoch 21/50\n",
      "8491/8491 [==============================] - 18s - loss: 2.3291 - mean_absolute_error: 0.2228    \n",
      "Epoch 22/50\n",
      "8491/8491 [==============================] - 18s - loss: 2.3149 - mean_absolute_error: 0.2316    \n",
      "Epoch 23/50\n",
      "8491/8491 [==============================] - 18s - loss: 2.2787 - mean_absolute_error: 0.2264    \n",
      "Epoch 24/50\n",
      "8491/8491 [==============================] - 18s - loss: 2.2108 - mean_absolute_error: 0.2062    \n",
      "Epoch 25/50\n",
      "8491/8491 [==============================] - 18s - loss: 2.1642 - mean_absolute_error: 0.2025    \n",
      "Epoch 26/50\n",
      "8491/8491 [==============================] - 19s - loss: 2.1347 - mean_absolute_error: 0.2035    \n",
      "Epoch 27/50\n",
      "8491/8491 [==============================] - 18s - loss: 2.1044 - mean_absolute_error: 0.2034    \n",
      "Epoch 28/50\n",
      "8491/8491 [==============================] - 18s - loss: 2.0848 - mean_absolute_error: 0.2073    \n",
      "Epoch 29/50\n",
      "8491/8491 [==============================] - 19s - loss: 2.0356 - mean_absolute_error: 0.1951    \n",
      "Epoch 30/50\n",
      "8491/8491 [==============================] - 19s - loss: 2.0075 - mean_absolute_error: 0.1983    \n",
      "Epoch 31/50\n",
      "8491/8491 [==============================] - 18s - loss: 1.9773 - mean_absolute_error: 0.1999    \n",
      "Epoch 32/50\n",
      "8491/8491 [==============================] - 19s - loss: 1.9457 - mean_absolute_error: 0.1931    \n",
      "Epoch 33/50\n",
      "8491/8491 [==============================] - 19s - loss: 1.9076 - mean_absolute_error: 0.1883    \n",
      "Epoch 34/50\n",
      "8491/8491 [==============================] - 18s - loss: 1.8828 - mean_absolute_error: 0.1874    \n",
      "Epoch 35/50\n",
      "8491/8491 [==============================] - 19s - loss: 1.8508 - mean_absolute_error: 0.1860    \n",
      "Epoch 36/50\n",
      "8491/8491 [==============================] - 19s - loss: 1.8179 - mean_absolute_error: 0.1812    \n",
      "Epoch 37/50\n",
      "8491/8491 [==============================] - 18s - loss: 1.7906 - mean_absolute_error: 0.1827    \n",
      "Epoch 38/50\n",
      "8491/8491 [==============================] - 19s - loss: 1.7672 - mean_absolute_error: 0.1821    \n",
      "Epoch 39/50\n",
      "8491/8491 [==============================] - 19s - loss: 1.7448 - mean_absolute_error: 0.1828    \n",
      "Epoch 40/50\n",
      "8491/8491 [==============================] - 19s - loss: 1.7189 - mean_absolute_error: 0.1836    \n",
      "Epoch 41/50\n",
      "8491/8491 [==============================] - 19s - loss: 1.6965 - mean_absolute_error: 0.1828    \n",
      "Epoch 42/50\n",
      "8491/8491 [==============================] - 19s - loss: 1.6724 - mean_absolute_error: 0.1822    \n",
      "Epoch 43/50\n",
      "8491/8491 [==============================] - 18s - loss: 1.6458 - mean_absolute_error: 0.1870    \n",
      "Epoch 44/50\n",
      "8491/8491 [==============================] - 19s - loss: 1.6211 - mean_absolute_error: 0.1839    \n",
      "Epoch 45/50\n",
      "8491/8491 [==============================] - 19s - loss: 1.5856 - mean_absolute_error: 0.1724    \n",
      "Epoch 46/50\n",
      "8491/8491 [==============================] - 19s - loss: 1.5656 - mean_absolute_error: 0.1748    \n",
      "Epoch 47/50\n",
      "8491/8491 [==============================] - 19s - loss: 1.5484 - mean_absolute_error: 0.1801    \n",
      "Epoch 48/50\n",
      "8491/8491 [==============================] - 18s - loss: 1.5417 - mean_absolute_error: 0.1954    \n",
      "Epoch 49/50\n",
      "8491/8491 [==============================] - 19s - loss: 1.5355 - mean_absolute_error: 0.1999    \n",
      "Epoch 50/50\n",
      "8491/8491 [==============================] - 19s - loss: 1.5091 - mean_absolute_error: 0.1922    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1df68cf50>"
      ]
     },
     "execution_count": 369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(2500, input_dim=MAX_NB_WORDS, init='normal', activation='relu',\\\n",
    "          W_regularizer=l2(0.0001), activity_regularizer=activity_l2(0.0001)))\n",
    "model.add(Dense(1000, init='normal', activation='relu',\\\n",
    "          W_regularizer=l2(0.0001), activity_regularizer=activity_l2(0.0001)))\n",
    "model.add(Dense(1000, init='normal', activation='relu',\\\n",
    "          W_regularizer=l2(0.0001), activity_regularizer=activity_l2(0.0001)))\n",
    "model.add(Dense(1, init='normal'))\n",
    "\n",
    "sgd = SGD(lr=0.002)\n",
    "\n",
    "\n",
    "model.compile(loss='mean_squared_error', metrics = ['mean_absolute_error'], optimizer='adam')\n",
    "model.fit(train_x, train_y, batch_size = 128, nb_epoch = 50 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>essay_set</th>\n",
       "      <th>essay</th>\n",
       "      <th>rater1_domain1</th>\n",
       "      <th>rater2_domain1</th>\n",
       "      <th>domain1_score</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>2979</td>\n",
       "      <td>2</td>\n",
       "      <td>Write a persuasive essay to a newspaper reflec...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2.399050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>2984</td>\n",
       "      <td>2</td>\n",
       "      <td>How @CAPS4 you feel if your favorite book was ...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3.906921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>2996</td>\n",
       "      <td>2</td>\n",
       "      <td>If the people that are publishing and writing ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.718492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>3029</td>\n",
       "      <td>2</td>\n",
       "      <td>wow thats racist. as i said when i saw the mov...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2.872141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>3033</td>\n",
       "      <td>2</td>\n",
       "      <td>Why should we be more carefull of what we get ...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3.931227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>3045</td>\n",
       "      <td>2</td>\n",
       "      <td>Should books, magazines, music, movies, and ec...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3.931227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>3099</td>\n",
       "      <td>2</td>\n",
       "      <td>Twilight, @PERSON1, or even The @CAPS1 @CAPS2 ...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3.935044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>3113</td>\n",
       "      <td>2</td>\n",
       "      <td>I dont believe books should be takin off of th...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2.765417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>3123</td>\n",
       "      <td>2</td>\n",
       "      <td>I believe censorship should be used in librari...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2.372568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>3126</td>\n",
       "      <td>2</td>\n",
       "      <td>Real life is rated @CAPS1 so why pretend it's ...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3.396649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>3155</td>\n",
       "      <td>2</td>\n",
       "      <td>The censorship in libraries should be as gog a...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3.182782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>3169</td>\n",
       "      <td>2</td>\n",
       "      <td>'And then we have no books left on the shelf f...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3.994154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>3189</td>\n",
       "      <td>2</td>\n",
       "      <td>Books have been apart of every single person's...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3.685853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>3192</td>\n",
       "      <td>2</td>\n",
       "      <td>When I go to a library I @MONTH1 find some stu...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2.501650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>3208</td>\n",
       "      <td>2</td>\n",
       "      <td>If I could remove anything from shelves of lib...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3.390728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>3219</td>\n",
       "      <td>2</td>\n",
       "      <td>I believe that libraries should provide any ki...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3.931227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>3226</td>\n",
       "      <td>2</td>\n",
       "      <td>Many places show and sell offensive material. ...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3.644776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>3243</td>\n",
       "      <td>2</td>\n",
       "      <td>Have you ever walked into the library hoping t...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3.931227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>3262</td>\n",
       "      <td>2</td>\n",
       "      <td>How many of you have ever heard any offensive ...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3.430997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>3265</td>\n",
       "      <td>2</td>\n",
       "      <td>A very wise man in @CAPS1 life once said, 'The...</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3.931227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>3269</td>\n",
       "      <td>2</td>\n",
       "      <td>Typically when you walk in to a library there ...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4.035143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>3285</td>\n",
       "      <td>2</td>\n",
       "      <td>I think that if certain materials are offensiv...</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2.196555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>3293</td>\n",
       "      <td>2</td>\n",
       "      <td>Recently the growing issue of censorship in li...</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>2.942841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>3294</td>\n",
       "      <td>2</td>\n",
       "      <td>I believe that if you hear or read something o...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3.698640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>3297</td>\n",
       "      <td>2</td>\n",
       "      <td>There are many books that are not appropriate ...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2.621563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>3298</td>\n",
       "      <td>2</td>\n",
       "      <td>Many people base their actions on their own vi...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3.946259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>3299</td>\n",
       "      <td>2</td>\n",
       "      <td>Many libraries contain books, movies, music, a...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3.931227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>3303</td>\n",
       "      <td>2</td>\n",
       "      <td>I'm writing this paper today to talk about cen...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2.283906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>3349</td>\n",
       "      <td>2</td>\n",
       "      <td>There are tons of books in the library that pe...</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3.958261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>3350</td>\n",
       "      <td>2</td>\n",
       "      <td>I don't think that they should be removed from...</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2.888950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>4527</td>\n",
       "      <td>2</td>\n",
       "      <td>Today it seems as if anything you could imagin...</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3.931227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>4543</td>\n",
       "      <td>2</td>\n",
       "      <td>People @MONTH1 have a certain reason to take s...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3.516143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>4557</td>\n",
       "      <td>2</td>\n",
       "      <td>Censorship in media has been around since the ...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2.797962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>4564</td>\n",
       "      <td>2</td>\n",
       "      <td>Everyone has there own opinions and arguements...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3.931227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>4574</td>\n",
       "      <td>2</td>\n",
       "      <td>Where it @CAPS1 @CAPS2 @CAPS3     @CAPS4 up no...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3.777409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>4575</td>\n",
       "      <td>2</td>\n",
       "      <td>Censorship is a two sided question. There are ...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2.809463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>4577</td>\n",
       "      <td>2</td>\n",
       "      <td>There are many things out in the 'material' wo...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3.931227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>4578</td>\n",
       "      <td>2</td>\n",
       "      <td>Controversy in the @CAPS1          @CAPS2 some...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2.597684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>4583</td>\n",
       "      <td>2</td>\n",
       "      <td>Nobody should believe that we have the choice ...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3.623025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>4597</td>\n",
       "      <td>2</td>\n",
       "      <td>BAD @CAPS1     Do you really want your kids to...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2.591640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>4602</td>\n",
       "      <td>2</td>\n",
       "      <td>How many of you out there absolutely love to r...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3.596990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>4607</td>\n",
       "      <td>2</td>\n",
       "      <td>Do yo think censorship is a good thing? Do you...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3.063461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>4617</td>\n",
       "      <td>2</td>\n",
       "      <td>Everyone has found something offensive through...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3.101567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>4626</td>\n",
       "      <td>2</td>\n",
       "      <td>Reading is something people choose to do for t...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3.623177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>4641</td>\n",
       "      <td>2</td>\n",
       "      <td>I think if its offensive it should not be avav...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2.566335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>4642</td>\n",
       "      <td>2</td>\n",
       "      <td>Dear Newspaper,     I would like to start off ...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3.542599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>4643</td>\n",
       "      <td>2</td>\n",
       "      <td>I am sending this letter to you about my views...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3.024898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>4652</td>\n",
       "      <td>2</td>\n",
       "      <td>Censorship to most people means much more than...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3.931227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311</th>\n",
       "      <td>4657</td>\n",
       "      <td>2</td>\n",
       "      <td>Some children take books off the shelf that so...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.161367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312</th>\n",
       "      <td>4658</td>\n",
       "      <td>2</td>\n",
       "      <td>Hello @ORGANIZATION1, I am writing to talk abo...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2.289247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313</th>\n",
       "      <td>4670</td>\n",
       "      <td>2</td>\n",
       "      <td>Yes I do believe that certain books, movies, m...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3.136159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314</th>\n",
       "      <td>4680</td>\n",
       "      <td>2</td>\n",
       "      <td>Censorship in Libraries     Have you ever came...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3.874950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315</th>\n",
       "      <td>4681</td>\n",
       "      <td>2</td>\n",
       "      <td>I personally do not think that books or magazi...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3.157877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>4696</td>\n",
       "      <td>2</td>\n",
       "      <td>Hi, my name is mason and i'm writing about wet...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2.702854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>4697</td>\n",
       "      <td>2</td>\n",
       "      <td>I am a student from @CAPS1. I wanted to let yo...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4.044975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>4705</td>\n",
       "      <td>2</td>\n",
       "      <td>Certain material that are made to offend peopl...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4.427194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>4717</td>\n",
       "      <td>2</td>\n",
       "      <td>I think that no books should be taken away. I ...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1.478512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>4722</td>\n",
       "      <td>2</td>\n",
       "      <td>In our socioty, a lot of people and places tak...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3.685657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321</th>\n",
       "      <td>4728</td>\n",
       "      <td>2</td>\n",
       "      <td>I do not think that there is a need to remove ...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3.361415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322</th>\n",
       "      <td>4751</td>\n",
       "      <td>2</td>\n",
       "      <td>I do not believe that certain materials, such ...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3.931227</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>169 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     essay_id  essay_set                                              essay  \\\n",
       "154      2979          2  Write a persuasive essay to a newspaper reflec...   \n",
       "155      2984          2  How @CAPS4 you feel if your favorite book was ...   \n",
       "156      2996          2  If the people that are publishing and writing ...   \n",
       "157      3029          2  wow thats racist. as i said when i saw the mov...   \n",
       "158      3033          2  Why should we be more carefull of what we get ...   \n",
       "159      3045          2  Should books, magazines, music, movies, and ec...   \n",
       "160      3099          2  Twilight, @PERSON1, or even The @CAPS1 @CAPS2 ...   \n",
       "161      3113          2  I dont believe books should be takin off of th...   \n",
       "162      3123          2  I believe censorship should be used in librari...   \n",
       "163      3126          2  Real life is rated @CAPS1 so why pretend it's ...   \n",
       "164      3155          2  The censorship in libraries should be as gog a...   \n",
       "165      3169          2  'And then we have no books left on the shelf f...   \n",
       "166      3189          2  Books have been apart of every single person's...   \n",
       "167      3192          2  When I go to a library I @MONTH1 find some stu...   \n",
       "168      3208          2  If I could remove anything from shelves of lib...   \n",
       "169      3219          2  I believe that libraries should provide any ki...   \n",
       "170      3226          2  Many places show and sell offensive material. ...   \n",
       "171      3243          2  Have you ever walked into the library hoping t...   \n",
       "172      3262          2  How many of you have ever heard any offensive ...   \n",
       "173      3265          2  A very wise man in @CAPS1 life once said, 'The...   \n",
       "174      3269          2  Typically when you walk in to a library there ...   \n",
       "175      3285          2  I think that if certain materials are offensiv...   \n",
       "176      3293          2  Recently the growing issue of censorship in li...   \n",
       "177      3294          2  I believe that if you hear or read something o...   \n",
       "178      3297          2  There are many books that are not appropriate ...   \n",
       "179      3298          2  Many people base their actions on their own vi...   \n",
       "180      3299          2  Many libraries contain books, movies, music, a...   \n",
       "181      3303          2  I'm writing this paper today to talk about cen...   \n",
       "182      3349          2  There are tons of books in the library that pe...   \n",
       "183      3350          2  I don't think that they should be removed from...   \n",
       "..        ...        ...                                                ...   \n",
       "293      4527          2  Today it seems as if anything you could imagin...   \n",
       "294      4543          2  People @MONTH1 have a certain reason to take s...   \n",
       "295      4557          2  Censorship in media has been around since the ...   \n",
       "296      4564          2  Everyone has there own opinions and arguements...   \n",
       "297      4574          2  Where it @CAPS1 @CAPS2 @CAPS3     @CAPS4 up no...   \n",
       "298      4575          2  Censorship is a two sided question. There are ...   \n",
       "299      4577          2  There are many things out in the 'material' wo...   \n",
       "300      4578          2  Controversy in the @CAPS1          @CAPS2 some...   \n",
       "301      4583          2  Nobody should believe that we have the choice ...   \n",
       "302      4597          2  BAD @CAPS1     Do you really want your kids to...   \n",
       "303      4602          2  How many of you out there absolutely love to r...   \n",
       "304      4607          2  Do yo think censorship is a good thing? Do you...   \n",
       "305      4617          2  Everyone has found something offensive through...   \n",
       "306      4626          2  Reading is something people choose to do for t...   \n",
       "307      4641          2  I think if its offensive it should not be avav...   \n",
       "308      4642          2  Dear Newspaper,     I would like to start off ...   \n",
       "309      4643          2  I am sending this letter to you about my views...   \n",
       "310      4652          2  Censorship to most people means much more than...   \n",
       "311      4657          2  Some children take books off the shelf that so...   \n",
       "312      4658          2  Hello @ORGANIZATION1, I am writing to talk abo...   \n",
       "313      4670          2  Yes I do believe that certain books, movies, m...   \n",
       "314      4680          2  Censorship in Libraries     Have you ever came...   \n",
       "315      4681          2  I personally do not think that books or magazi...   \n",
       "316      4696          2  Hi, my name is mason and i'm writing about wet...   \n",
       "317      4697          2  I am a student from @CAPS1. I wanted to let yo...   \n",
       "318      4705          2  Certain material that are made to offend peopl...   \n",
       "319      4717          2  I think that no books should be taken away. I ...   \n",
       "320      4722          2  In our socioty, a lot of people and places tak...   \n",
       "321      4728          2  I do not think that there is a need to remove ...   \n",
       "322      4751          2  I do not believe that certain materials, such ...   \n",
       "\n",
       "     rater1_domain1  rater2_domain1  domain1_score  prediction  \n",
       "154               1               2              1    2.399050  \n",
       "155               5               5              5    3.906921  \n",
       "156               1               1              1    2.718492  \n",
       "157               3               3              3    2.872141  \n",
       "158               4               4              4    3.931227  \n",
       "159               4               4              4    3.931227  \n",
       "160               4               4              4    3.935044  \n",
       "161               3               3              3    2.765417  \n",
       "162               4               4              4    2.372568  \n",
       "163               4               4              4    3.396649  \n",
       "164               3               3              3    3.182782  \n",
       "165               3               3              3    3.994154  \n",
       "166               4               4              4    3.685853  \n",
       "167               3               3              3    2.501650  \n",
       "168               4               4              4    3.390728  \n",
       "169               4               4              4    3.931227  \n",
       "170               4               4              4    3.644776  \n",
       "171               4               4              4    3.931227  \n",
       "172               4               4              4    3.430997  \n",
       "173               4               5              4    3.931227  \n",
       "174               4               4              4    4.035143  \n",
       "175               4               3              4    2.196555  \n",
       "176               4               5              4    2.942841  \n",
       "177               4               4              4    3.698640  \n",
       "178               3               3              3    2.621563  \n",
       "179               4               4              4    3.946259  \n",
       "180               4               4              4    3.931227  \n",
       "181               3               3              3    2.283906  \n",
       "182               3               4              3    3.958261  \n",
       "183               3               4              3    2.888950  \n",
       "..              ...             ...            ...         ...  \n",
       "293               4               3              4    3.931227  \n",
       "294               4               4              4    3.516143  \n",
       "295               4               4              4    2.797962  \n",
       "296               3               3              3    3.931227  \n",
       "297               4               4              4    3.777409  \n",
       "298               4               4              4    2.809463  \n",
       "299               5               5              5    3.931227  \n",
       "300               4               4              4    2.597684  \n",
       "301               3               3              3    3.623025  \n",
       "302               3               3              3    2.591640  \n",
       "303               4               4              4    3.596990  \n",
       "304               3               3              3    3.063461  \n",
       "305               4               4              4    3.101567  \n",
       "306               4               4              4    3.623177  \n",
       "307               3               3              3    2.566335  \n",
       "308               3               3              3    3.542599  \n",
       "309               3               3              3    3.024898  \n",
       "310               4               4              4    3.931227  \n",
       "311               1               1              1    1.161367  \n",
       "312               3               3              3    2.289247  \n",
       "313               3               3              3    3.136159  \n",
       "314               4               4              4    3.874950  \n",
       "315               3               3              3    3.157877  \n",
       "316               3               3              3    2.702854  \n",
       "317               4               4              4    4.044975  \n",
       "318               4               4              4    4.427194  \n",
       "319               2               2              2    1.478512  \n",
       "320               2               2              2    3.685657  \n",
       "321               4               4              4    3.361415  \n",
       "322               4               4              4    3.931227  \n",
       "\n",
       "[169 rows x 7 columns]"
      ]
     },
     "execution_count": 370,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_y = model.predict(test_x)\n",
    "test[\"prediction\"] = pred_y\n",
    "test[test[\"essay_set\"] == 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kappa for each set [ 0.9573843   0.91683706  0.96798999  0.98729912  0.98625968  0.98133063]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.97311439697647051"
      ]
     },
     "execution_count": 371,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regress_kappa(labels_train,model.predict(train_x),train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kappa for each set [ 0.48131304  0.52971314  0.47251242  0.69335093  0.70572218  0.594049  ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.58767347881919696"
      ]
     },
     "execution_count": 372,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regress_kappa(labels_test,model.predict(test_x),test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tfdif regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28587 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "MAX_NB_WORDS = 1000\n",
    "tokenizer = keras.preprocessing.text.Tokenizer(nb_words = MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(texts_train + texts_test)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_x = tokenizer.texts_to_matrix(texts_train, mode = 'tfidf')\n",
    "test_x = tokenizer.texts_to_matrix(texts_test, mode = 'tfidf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_y = to_categorical(labels_train)\n",
    "test_y = to_categorical(labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_x = np.hstack((train_x, to_categorical(train[\"essay_set\"].values)))\n",
    "test_x = np.hstack((test_x, to_categorical(test[\"essay_set\"].values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "8491/8491 [==============================] - 5s - loss: 4.0017 - acc: 0.3132     \n",
      "Epoch 2/50\n",
      "8491/8491 [==============================] - 3s - loss: 1.9974 - acc: 0.4904     \n",
      "Epoch 3/50\n",
      "8491/8491 [==============================] - 3s - loss: 1.5503 - acc: 0.7111     \n",
      "Epoch 4/50\n",
      "8491/8491 [==============================] - 3s - loss: 1.2658 - acc: 0.8072     \n",
      "Epoch 5/50\n",
      "8491/8491 [==============================] - 3s - loss: 1.0969 - acc: 0.8536     \n",
      "Epoch 6/50\n",
      "8491/8491 [==============================] - 4s - loss: 0.9802 - acc: 0.8719     \n",
      "Epoch 7/50\n",
      "8491/8491 [==============================] - 3s - loss: 0.8922 - acc: 0.8780     \n",
      "Epoch 8/50\n",
      "8491/8491 [==============================] - 3s - loss: 0.8249 - acc: 0.8832     \n",
      "Epoch 9/50\n",
      "8491/8491 [==============================] - 4s - loss: 0.7592 - acc: 0.8911     \n",
      "Epoch 10/50\n",
      "8491/8491 [==============================] - 4s - loss: 0.7128 - acc: 0.8961     \n",
      "Epoch 11/50\n",
      "8491/8491 [==============================] - 3s - loss: 0.6794 - acc: 0.9025     \n",
      "Epoch 12/50\n",
      "8491/8491 [==============================] - 3s - loss: 0.6650 - acc: 0.9019     \n",
      "Epoch 13/50\n",
      "8491/8491 [==============================] - 3s - loss: 0.6850 - acc: 0.8919     \n",
      "Epoch 14/50\n",
      "8491/8491 [==============================] - 3s - loss: 0.7148 - acc: 0.8812     \n",
      "Epoch 15/50\n",
      "8491/8491 [==============================] - 3s - loss: 0.7514 - acc: 0.8722     \n",
      "Epoch 16/50\n",
      "8491/8491 [==============================] - 4s - loss: 0.6912 - acc: 0.8966     \n",
      "Epoch 17/50\n",
      "8491/8491 [==============================] - 3s - loss: 0.6312 - acc: 0.9105     \n",
      "Epoch 18/50\n",
      "8491/8491 [==============================] - 3s - loss: 0.5959 - acc: 0.9164     \n",
      "Epoch 19/50\n",
      "8491/8491 [==============================] - 3s - loss: 0.5580 - acc: 0.9186     \n",
      "Epoch 20/50\n",
      "8491/8491 [==============================] - 3s - loss: 0.5334 - acc: 0.9237     \n",
      "Epoch 21/50\n",
      "8491/8491 [==============================] - 3s - loss: 0.5088 - acc: 0.9233     \n",
      "Epoch 22/50\n",
      "8491/8491 [==============================] - 3s - loss: 0.4912 - acc: 0.9275     \n",
      "Epoch 23/50\n",
      "8491/8491 [==============================] - 3s - loss: 0.4709 - acc: 0.9279     \n",
      "Epoch 24/50\n",
      "8491/8491 [==============================] - 3s - loss: 0.4533 - acc: 0.9279     \n",
      "Epoch 25/50\n",
      "8491/8491 [==============================] - 3s - loss: 0.4377 - acc: 0.9296     \n",
      "Epoch 26/50\n",
      "8491/8491 [==============================] - 3s - loss: 0.4346 - acc: 0.9276     \n",
      "Epoch 27/50\n",
      "8491/8491 [==============================] - 3s - loss: 0.4318 - acc: 0.9257     \n",
      "Epoch 28/50\n",
      "8491/8491 [==============================] - 3s - loss: 0.4157 - acc: 0.9276     \n",
      "Epoch 29/50\n",
      "8491/8491 [==============================] - 3s - loss: 0.4380 - acc: 0.9229     \n",
      "Epoch 30/50\n",
      "8491/8491 [==============================] - 3s - loss: 0.4482 - acc: 0.9191     \n",
      "Epoch 31/50\n",
      "8491/8491 [==============================] - 3s - loss: 0.4626 - acc: 0.9141     \n",
      "Epoch 32/50\n",
      "8491/8491 [==============================] - 3s - loss: 0.4936 - acc: 0.9108     \n",
      "Epoch 33/50\n",
      "8491/8491 [==============================] - 3s - loss: 0.5315 - acc: 0.9007     \n",
      "Epoch 34/50\n",
      "8491/8491 [==============================] - 3s - loss: 0.5208 - acc: 0.9041     \n",
      "Epoch 35/50\n",
      "8491/8491 [==============================] - 3s - loss: 0.4732 - acc: 0.9200     \n",
      "Epoch 36/50\n",
      "8491/8491 [==============================] - 3s - loss: 0.4516 - acc: 0.9255     \n",
      "Epoch 37/50\n",
      "8491/8491 [==============================] - 3s - loss: 0.4254 - acc: 0.9299     \n",
      "Epoch 38/50\n",
      "8491/8491 [==============================] - 3s - loss: 0.4018 - acc: 0.9335     \n",
      "Epoch 39/50\n",
      "8491/8491 [==============================] - 3s - loss: 0.3943 - acc: 0.9317     \n",
      "Epoch 40/50\n",
      "8491/8491 [==============================] - 3s - loss: 0.3847 - acc: 0.9332     \n",
      "Epoch 41/50\n",
      "8491/8491 [==============================] - 3s - loss: 0.3745 - acc: 0.9323     \n",
      "Epoch 42/50\n",
      "8491/8491 [==============================] - 3s - loss: 0.3707 - acc: 0.9316     \n",
      "Epoch 43/50\n",
      "8491/8491 [==============================] - 3s - loss: 0.3588 - acc: 0.9325     \n",
      "Epoch 44/50\n",
      "8491/8491 [==============================] - 3s - loss: 0.3501 - acc: 0.9337     \n",
      "Epoch 45/50\n",
      "8491/8491 [==============================] - 3s - loss: 0.3396 - acc: 0.9340     \n",
      "Epoch 46/50\n",
      "8491/8491 [==============================] - 3s - loss: 0.3406 - acc: 0.9318     \n",
      "Epoch 47/50\n",
      "8491/8491 [==============================] - 3s - loss: 0.3428 - acc: 0.9335     \n",
      "Epoch 48/50\n",
      "8491/8491 [==============================] - 3s - loss: 0.3599 - acc: 0.9277     \n",
      "Epoch 49/50\n",
      "8491/8491 [==============================] - 3s - loss: 0.3626 - acc: 0.9253     \n",
      "Epoch 50/50\n",
      "8491/8491 [==============================] - 3s - loss: 0.3821 - acc: 0.9231     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x218d0ccd0>"
      ]
     },
     "execution_count": 410,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(1000, input_dim=MAX_NB_WORDS, init='normal', activation='relu',\\\n",
    "          W_regularizer=l2(0.0001), activity_regularizer=activity_l2(0.0001)))\n",
    "model.add(Dense(1000, init='normal', activation='relu',\\\n",
    "          W_regularizer=l2(0.0001), activity_regularizer=activity_l2(0.0001)))\n",
    "model.add(Dense(1000, init='normal', activation='relu',\\\n",
    "          W_regularizer=l2(0.0001), activity_regularizer=activity_l2(0.0001)))\n",
    "\n",
    "model.add(Dense(13, init='normal', activation='softmax'))\n",
    "sgd = SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(train_x, train_y, batch_size = 128, nb_epoch = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1052/1052 [==============================] - 1s     \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>essay_set</th>\n",
       "      <th>essay</th>\n",
       "      <th>rater1_domain1</th>\n",
       "      <th>rater2_domain1</th>\n",
       "      <th>domain1_score</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear Local Newspaper, @CAPS1 I have found that...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear Local Newspaper, I belive that computers ...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>46</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear, Local Newspaper @CAPS1 here to inform yo...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>55</td>\n",
       "      <td>1</td>\n",
       "      <td>ARE YOU @CAPS1!! Computers are great, they're ...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>61</td>\n",
       "      <td>1</td>\n",
       "      <td>Do you spend all or most of your freetime sitt...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>69</td>\n",
       "      <td>1</td>\n",
       "      <td>Some people think it is a good idea and same d...</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>87</td>\n",
       "      <td>1</td>\n",
       "      <td>Computers. One of the much enjoyed pieces of t...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>108</td>\n",
       "      <td>1</td>\n",
       "      <td>I saw in one of the news papers I got in the m...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>127</td>\n",
       "      <td>1</td>\n",
       "      <td>Computers can affect the way people are and ho...</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>160</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear local newspaper, I agree that people are ...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>165</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear Local Newspaper, everday @CAPS1 technolog...</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>168</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @LOCATION1 Newspaper: @CAPS1 you really t...</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>173</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear, Newspaper I would like to tell you about...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>176</td>\n",
       "      <td>1</td>\n",
       "      <td>Computers and the @CAPS1 were a technological ...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>181</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear Readers of the @ORGANIZATION1, Computers ...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>218</td>\n",
       "      <td>1</td>\n",
       "      <td>@ORGANIZATION1, @CAPS1 you @CAPS2 want your ki...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>247</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear Newspaper @CAPS1, @CAPS2 people are now u...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>263</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @ORGANIZATION1, @CAPS1 minute of the day ...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>275</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @PERSON1, Advances in technology and comp...</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>276</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear local Newspaper, I believe that computers...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>291</td>\n",
       "      <td>1</td>\n",
       "      <td>Have you ever thought about all the amazing th...</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>299</td>\n",
       "      <td>1</td>\n",
       "      <td>Do you think computers have a negative effect ...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>309</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @CAPS1 @CAPS2, @CAPS3 @CAPS4 all the talk...</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>342</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear editor: More and more people use computer...</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>357</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear local newspaper, I am writing to you beca...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>377</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear Readers of the @ORGANIZATION1, @CAPS1 you...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>384</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @CAPS1, @CAPS2 you think computers have a...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>404</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear Local Newspaper, The effects computers ha...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>409</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear local newspaper, @CAPS1 name is @PERSON1....</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>429</td>\n",
       "      <td>1</td>\n",
       "      <td>@ORGANIZATION1, Computers are great tools and ...</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>1462</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear Local Newspaper, @CAPS1 a world where the...</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>1473</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear local news paper, This paper is going to ...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>1489</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear local newspaper, In my opinion I feel lik...</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>1502</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear local newspaper, I believe that with too ...</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>1508</td>\n",
       "      <td>1</td>\n",
       "      <td>Computers. Theres fun. But there's also a down...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>1511</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @ORGANIZATION1, The computer has been a r...</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>1530</td>\n",
       "      <td>1</td>\n",
       "      <td>To: @ORGANIZATION1 goes so fast, and the most ...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>1544</td>\n",
       "      <td>1</td>\n",
       "      <td>I People spend too much time on the computer. ...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>1551</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear local newspaper- I understand that comput...</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>1558</td>\n",
       "      <td>1</td>\n",
       "      <td>@PERCENT1 of all people in the @LOCATION3 have...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>1562</td>\n",
       "      <td>1</td>\n",
       "      <td>The importance of computers in the modern worl...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>1563</td>\n",
       "      <td>1</td>\n",
       "      <td>Computers, one of the daily things we use and ...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>1575</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear Local Newspaper, I am writing to you to t...</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>1578</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear, @LOCATION1 @ORGANIZATION1 I think that a...</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>1594</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @CAPS1, many people say that computers ca...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>1596</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @CAPS1 @CAPS2, I agree that computers had...</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>1603</td>\n",
       "      <td>1</td>\n",
       "      <td>To the local Newspaper, I think using computer...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>1605</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear local newspaper, I think the effects of c...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>1644</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear local newspaper, I strongly believe compu...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>1665</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @CAPS1, Advances in computer technology h...</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>1673</td>\n",
       "      <td>1</td>\n",
       "      <td>Everyone will agree that using computers is al...</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>1703</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @CAPS1 @CAPS2 @CAPS3, The effects compute...</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>1705</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @CAPS1 @CAPS2, @CAPS3, walking into a roo...</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>1717</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @CAPS1, @CAPS2 name is @PERSON1 and I wan...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>1719</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear Local Newspaper, @CAPS1 about what the wo...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>1722</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear local newspaper, @CAPS1 people are talkin...</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>1733</td>\n",
       "      <td>1</td>\n",
       "      <td>I think that computer are a good benefit for p...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>1742</td>\n",
       "      <td>1</td>\n",
       "      <td>To whom it @MONTH1 concern; I am writing this ...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>1763</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear Local Newspaper: @CAPS1 you know that ove...</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>1785</td>\n",
       "      <td>1</td>\n",
       "      <td>My opinion is that people should have computer...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>154 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     essay_id  essay_set                                              essay  \\\n",
       "0           4          1  Dear Local Newspaper, @CAPS1 I have found that...   \n",
       "1          17          1  Dear Local Newspaper, I belive that computers ...   \n",
       "2          46          1  Dear, Local Newspaper @CAPS1 here to inform yo...   \n",
       "3          55          1  ARE YOU @CAPS1!! Computers are great, they're ...   \n",
       "4          61          1  Do you spend all or most of your freetime sitt...   \n",
       "5          69          1  Some people think it is a good idea and same d...   \n",
       "6          87          1  Computers. One of the much enjoyed pieces of t...   \n",
       "7         108          1  I saw in one of the news papers I got in the m...   \n",
       "8         127          1  Computers can affect the way people are and ho...   \n",
       "9         160          1  Dear local newspaper, I agree that people are ...   \n",
       "10        165          1  Dear Local Newspaper, everday @CAPS1 technolog...   \n",
       "11        168          1  Dear @LOCATION1 Newspaper: @CAPS1 you really t...   \n",
       "12        173          1  Dear, Newspaper I would like to tell you about...   \n",
       "13        176          1  Computers and the @CAPS1 were a technological ...   \n",
       "14        181          1  Dear Readers of the @ORGANIZATION1, Computers ...   \n",
       "15        218          1  @ORGANIZATION1, @CAPS1 you @CAPS2 want your ki...   \n",
       "16        247          1  Dear Newspaper @CAPS1, @CAPS2 people are now u...   \n",
       "17        263          1  Dear @ORGANIZATION1, @CAPS1 minute of the day ...   \n",
       "18        275          1  Dear @PERSON1, Advances in technology and comp...   \n",
       "19        276          1  Dear local Newspaper, I believe that computers...   \n",
       "20        291          1  Have you ever thought about all the amazing th...   \n",
       "21        299          1  Do you think computers have a negative effect ...   \n",
       "22        309          1  Dear @CAPS1 @CAPS2, @CAPS3 @CAPS4 all the talk...   \n",
       "23        342          1  Dear editor: More and more people use computer...   \n",
       "24        357          1  Dear local newspaper, I am writing to you beca...   \n",
       "25        377          1  Dear Readers of the @ORGANIZATION1, @CAPS1 you...   \n",
       "26        384          1  Dear @CAPS1, @CAPS2 you think computers have a...   \n",
       "27        404          1  Dear Local Newspaper, The effects computers ha...   \n",
       "28        409          1  Dear local newspaper, @CAPS1 name is @PERSON1....   \n",
       "29        429          1  @ORGANIZATION1, Computers are great tools and ...   \n",
       "..        ...        ...                                                ...   \n",
       "124      1462          1  Dear Local Newspaper, @CAPS1 a world where the...   \n",
       "125      1473          1  Dear local news paper, This paper is going to ...   \n",
       "126      1489          1  Dear local newspaper, In my opinion I feel lik...   \n",
       "127      1502          1  Dear local newspaper, I believe that with too ...   \n",
       "128      1508          1  Computers. Theres fun. But there's also a down...   \n",
       "129      1511          1  Dear @ORGANIZATION1, The computer has been a r...   \n",
       "130      1530          1  To: @ORGANIZATION1 goes so fast, and the most ...   \n",
       "131      1544          1  I People spend too much time on the computer. ...   \n",
       "132      1551          1  Dear local newspaper- I understand that comput...   \n",
       "133      1558          1  @PERCENT1 of all people in the @LOCATION3 have...   \n",
       "134      1562          1  The importance of computers in the modern worl...   \n",
       "135      1563          1  Computers, one of the daily things we use and ...   \n",
       "136      1575          1  Dear Local Newspaper, I am writing to you to t...   \n",
       "137      1578          1  Dear, @LOCATION1 @ORGANIZATION1 I think that a...   \n",
       "138      1594          1  Dear @CAPS1, many people say that computers ca...   \n",
       "139      1596          1  Dear @CAPS1 @CAPS2, I agree that computers had...   \n",
       "140      1603          1  To the local Newspaper, I think using computer...   \n",
       "141      1605          1  Dear local newspaper, I think the effects of c...   \n",
       "142      1644          1  Dear local newspaper, I strongly believe compu...   \n",
       "143      1665          1  Dear @CAPS1, Advances in computer technology h...   \n",
       "144      1673          1  Everyone will agree that using computers is al...   \n",
       "145      1703          1  Dear @CAPS1 @CAPS2 @CAPS3, The effects compute...   \n",
       "146      1705          1  Dear @CAPS1 @CAPS2, @CAPS3, walking into a roo...   \n",
       "147      1717          1  Dear @CAPS1, @CAPS2 name is @PERSON1 and I wan...   \n",
       "148      1719          1  Dear Local Newspaper, @CAPS1 about what the wo...   \n",
       "149      1722          1  Dear local newspaper, @CAPS1 people are talkin...   \n",
       "150      1733          1  I think that computer are a good benefit for p...   \n",
       "151      1742          1  To whom it @MONTH1 concern; I am writing this ...   \n",
       "152      1763          1  Dear Local Newspaper: @CAPS1 you know that ove...   \n",
       "153      1785          1  My opinion is that people should have computer...   \n",
       "\n",
       "     rater1_domain1  rater2_domain1  domain1_score  prediction  \n",
       "0                 5               5             10          10  \n",
       "1                 4               4              8           8  \n",
       "2                 4               4              8           2  \n",
       "3                 4               4              8           8  \n",
       "4                 4               4              8          10  \n",
       "5                 3               4              7           8  \n",
       "6                 5               5             10          10  \n",
       "7                 4               4              8          10  \n",
       "8                 4               5              9          10  \n",
       "9                 4               4              8           8  \n",
       "10                4               5              9          10  \n",
       "11                5               4              9          10  \n",
       "12                4               4              8          10  \n",
       "13                5               5             10          10  \n",
       "14                5               5             10          10  \n",
       "15                4               4              8          10  \n",
       "16                4               4              8           8  \n",
       "17                5               5             10           8  \n",
       "18                6               5             11          10  \n",
       "19                4               4              8          10  \n",
       "20                5               6             11          10  \n",
       "21                4               4              8          10  \n",
       "22                4               5              9          10  \n",
       "23                5               4              9          10  \n",
       "24                4               4              8           8  \n",
       "25                5               5             10          10  \n",
       "26                4               4              8           8  \n",
       "27                4               4              8          10  \n",
       "28                4               4              8           8  \n",
       "29                5               4              9          10  \n",
       "..              ...             ...            ...         ...  \n",
       "124               4               3              7           8  \n",
       "125               3               3              6           8  \n",
       "126               5               4              9          10  \n",
       "127               6               4             10          10  \n",
       "128               4               4              8          10  \n",
       "129               4               3              7           8  \n",
       "130               4               4              8          10  \n",
       "131               5               5             10           9  \n",
       "132               4               5              9          10  \n",
       "133               4               4              8          10  \n",
       "134               2               2              4           8  \n",
       "135               3               3              6           7  \n",
       "136               4               5              9          10  \n",
       "137               4               5              9          10  \n",
       "138               4               4              8          10  \n",
       "139               4               5              9           8  \n",
       "140               4               4              8          10  \n",
       "141               4               4              8           8  \n",
       "142               4               4              8           8  \n",
       "143               4               5              9          10  \n",
       "144               2               3              5           1  \n",
       "145               5               6             11          10  \n",
       "146               6               6             12          10  \n",
       "147               4               4              8          10  \n",
       "148               4               4              8          10  \n",
       "149               4               5              9           9  \n",
       "150               4               4              8           8  \n",
       "151               3               3              6          10  \n",
       "152               6               6             12          10  \n",
       "153               4               4              8           8  \n",
       "\n",
       "[154 rows x 7 columns]"
      ]
     },
     "execution_count": 411,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_y = model.predict_classes(test_x)\n",
    "test[\"prediction\"] = pred_y\n",
    "test[test[\"essay_set\"] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kappa for each set [ 0.8397268   0.72055908  0.98118579  0.97978893  0.88684094  0.97170524]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.93702781596423268"
      ]
     },
     "execution_count": 412,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_kappa(train_y, model.predict(train_x), train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kappa for each set [ 0.51761489  0.09874275  0.09022382  0.17894521  0.18033696  0.22126791]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.22140911534217456"
      ]
     },
     "execution_count": 413,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_kappa(test_y, model.predict(test_x), test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "MAX_NB_WORDS = 5000\n",
    "MAX_SEQUENCE_LENGTH = 700\n",
    "EMBEDDING_DIM = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 35617 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = keras.preprocessing.text.Tokenizer(nb_words = MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(texts_train + texts_test)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_x = tokenizer.texts_to_sequences(texts_train)\n",
    "test_x = tokenizer.texts_to_sequences(texts_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Shape of data tensor:', (10314, 700))\n"
     ]
    }
   ],
   "source": [
    "train_x = pad_sequences(train_x, padding = 'post', truncating = 'post', maxlen=MAX_SEQUENCE_LENGTH)\n",
    "test_x = pad_sequences(test_x, padding = 'post',truncating = 'post', maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "print('Shape of data tensor:', train_x.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inv_map = {v: k for k, v in word_index.iteritems()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'caps',\n",
       " 2: 'people',\n",
       " 3: 'would',\n",
       " 4: 'computers',\n",
       " 5: 'time',\n",
       " 6: 'like',\n",
       " 7: 'one',\n",
       " 8: 'person',\n",
       " 9: 'computer',\n",
       " 10: 'get',\n",
       " 11: 'could',\n",
       " 12: 'also',\n",
       " 13: 'books',\n",
       " 14: 'think',\n",
       " 15: 'building',\n",
       " 16: 'num',\n",
       " 17: 'many',\n",
       " 18: 'things',\n",
       " 19: 'go',\n",
       " 20: 'family',\n",
       " 21: 'book',\n",
       " 22: 'way',\n",
       " 23: 'even',\n",
       " 24: 'author',\n",
       " 25: 'life',\n",
       " 26: 'parents',\n",
       " 27: 'know',\n",
       " 28: 'make',\n",
       " 29: 'friends',\n",
       " 30: 'good',\n",
       " 31: 'going',\n",
       " 32: 'offensive',\n",
       " 33: 'story',\n",
       " 34: 'want',\n",
       " 35: 'take',\n",
       " 36: 'us',\n",
       " 37: 'read',\n",
       " 38: 'see',\n",
       " 39: 'home',\n",
       " 40: 'day',\n",
       " 41: 'something',\n",
       " 42: 'new',\n",
       " 43: 'said',\n",
       " 44: 'mood',\n",
       " 45: 'much',\n",
       " 46: 'library',\n",
       " 47: 'got',\n",
       " 48: 'back',\n",
       " 49: 'dirigibles',\n",
       " 50: 'state',\n",
       " 51: 'use',\n",
       " 52: 'location',\n",
       " 53: 'cyclist',\n",
       " 54: 'another',\n",
       " 55: 'children',\n",
       " 56: 'organization',\n",
       " 57: 'help',\n",
       " 58: 'find',\n",
       " 59: 'music',\n",
       " 60: 'always',\n",
       " 61: 'around',\n",
       " 62: 'thing',\n",
       " 63: 'need',\n",
       " 64: 'empire',\n",
       " 65: 'really',\n",
       " 66: 'say',\n",
       " 67: 'world',\n",
       " 68: 'first',\n",
       " 69: 'kids',\n",
       " 70: 'right',\n",
       " 71: 'bad',\n",
       " 72: 'made',\n",
       " 73: 'libraries',\n",
       " 74: 'month',\n",
       " 75: 'mast',\n",
       " 76: 'movies',\n",
       " 77: 'reason',\n",
       " 78: 'patient',\n",
       " 79: 'someone',\n",
       " 80: 'paragraph',\n",
       " 81: 'everyone',\n",
       " 82: 'never',\n",
       " 83: 'school',\n",
       " 84: 'water',\n",
       " 85: 'away',\n",
       " 86: 'learn',\n",
       " 87: 'different',\n",
       " 88: 'talk',\n",
       " 89: 'went',\n",
       " 90: 'test',\n",
       " 91: 'saeng',\n",
       " 92: 'love',\n",
       " 93: 'lot',\n",
       " 94: 'example',\n",
       " 95: 'great',\n",
       " 96: 'builders',\n",
       " 97: 'believe',\n",
       " 98: 'memoir',\n",
       " 99: 'laughter',\n",
       " 100: 'happy',\n",
       " 101: 'setting',\n",
       " 102: 'still',\n",
       " 103: 'come',\n",
       " 104: 'mooring',\n",
       " 105: 'well',\n",
       " 106: 'better',\n",
       " 107: 'feel',\n",
       " 108: 'house',\n",
       " 109: 'dirigible',\n",
       " 110: 'laugh',\n",
       " 111: 'work',\n",
       " 112: 'says',\n",
       " 113: 'every',\n",
       " 114: 'able',\n",
       " 115: 'faced',\n",
       " 116: 'obstacles',\n",
       " 117: 'look',\n",
       " 118: 'little',\n",
       " 119: 'mom',\n",
       " 120: 'place',\n",
       " 121: 'dock',\n",
       " 122: 'date',\n",
       " 123: 'keep',\n",
       " 124: 'used',\n",
       " 125: 'ever',\n",
       " 126: 'came',\n",
       " 127: 'put',\n",
       " 128: 'narciso',\n",
       " 129: 'long',\n",
       " 130: 'magazines',\n",
       " 131: 'might',\n",
       " 132: 'reading',\n",
       " 133: 'hibiscus',\n",
       " 134: 'allow',\n",
       " 135: 'certain',\n",
       " 136: 'without',\n",
       " 137: 'top',\n",
       " 138: 'information',\n",
       " 139: 'give',\n",
       " 140: 'laughing',\n",
       " 141: 'everything',\n",
       " 142: 'hard',\n",
       " 143: 'important',\n",
       " 144: 'shows',\n",
       " 145: 'old',\n",
       " 146: 'online',\n",
       " 147: 'next',\n",
       " 148: 'shelves',\n",
       " 149: 'two',\n",
       " 150: 'started',\n",
       " 151: 'getting',\n",
       " 152: 'censorship',\n",
       " 153: 'fun',\n",
       " 154: 'let',\n",
       " 155: 'child',\n",
       " 156: 'last',\n",
       " 157: 'found',\n",
       " 158: 'nature',\n",
       " 159: 'society',\n",
       " 160: 'anything',\n",
       " 161: 'friend',\n",
       " 162: 'problem',\n",
       " 163: 'outside',\n",
       " 164: 'finally',\n",
       " 165: 'thought',\n",
       " 166: 'obstacle',\n",
       " 167: 'hand',\n",
       " 168: 'told',\n",
       " 169: 'others',\n",
       " 170: 'best',\n",
       " 171: 'wanted',\n",
       " 172: 'tell',\n",
       " 173: 'internet',\n",
       " 174: 'material',\n",
       " 175: 'show',\n",
       " 176: 'movie',\n",
       " 177: 'created',\n",
       " 178: 'patience',\n",
       " 179: 'end',\n",
       " 180: 'though',\n",
       " 181: 'dont',\n",
       " 182: 'makes',\n",
       " 183: 'hydrogen',\n",
       " 184: 'watch',\n",
       " 185: 'trying',\n",
       " 186: 'together',\n",
       " 187: 'wait',\n",
       " 188: 'air',\n",
       " 189: 'materials',\n",
       " 190: 'talking',\n",
       " 191: 'left',\n",
       " 192: 'play',\n",
       " 193: 'effect',\n",
       " 194: 'spend',\n",
       " 195: 'start',\n",
       " 196: 'big',\n",
       " 197: 'games',\n",
       " 198: 'nothing',\n",
       " 199: 'grateful',\n",
       " 200: 'stay',\n",
       " 201: 'took',\n",
       " 202: 'idea',\n",
       " 203: 'shelf',\n",
       " 204: 'try',\n",
       " 205: 'done',\n",
       " 206: 'enough',\n",
       " 207: 'times',\n",
       " 208: 'law',\n",
       " 209: 'dear',\n",
       " 210: 'age',\n",
       " 211: 'ways',\n",
       " 212: 'lives',\n",
       " 213: 'taken',\n",
       " 214: 'dad',\n",
       " 215: 'features',\n",
       " 216: 'saying',\n",
       " 217: 'making',\n",
       " 218: 'reasons',\n",
       " 219: 'affect',\n",
       " 220: 'using',\n",
       " 221: 'else',\n",
       " 222: 'states',\n",
       " 223: 'removed',\n",
       " 224: 'hope',\n",
       " 225: 'cause',\n",
       " 226: 'almost',\n",
       " 227: 'taking',\n",
       " 228: 'hours',\n",
       " 229: 'type',\n",
       " 230: 'rodriguez',\n",
       " 231: 'eye',\n",
       " 232: 'instead',\n",
       " 233: 'country',\n",
       " 234: 'live',\n",
       " 235: 'area',\n",
       " 236: 'face',\n",
       " 237: 'wrong',\n",
       " 238: 'winds',\n",
       " 239: 'opinion',\n",
       " 240: 'stop',\n",
       " 241: 'problems',\n",
       " 242: 'road',\n",
       " 243: 'whole',\n",
       " 244: 'today',\n",
       " 245: 'looking',\n",
       " 246: 'mean',\n",
       " 247: 'positive',\n",
       " 248: 'money',\n",
       " 249: 'due',\n",
       " 250: 'high',\n",
       " 251: 'room',\n",
       " 252: 'kind',\n",
       " 253: 'places',\n",
       " 254: 'part',\n",
       " 255: 'stuff',\n",
       " 256: 'waiting',\n",
       " 257: 'young',\n",
       " 258: 'sometimes',\n",
       " 259: 'ride',\n",
       " 260: 'year',\n",
       " 261: 'less',\n",
       " 262: 'conclusion',\n",
       " 263: 'line',\n",
       " 264: 'become',\n",
       " 265: 'percent',\n",
       " 266: 'listen',\n",
       " 267: 'spring',\n",
       " 268: 'concludes',\n",
       " 269: 'mother',\n",
       " 270: 'low',\n",
       " 271: 'since',\n",
       " 272: 'hot',\n",
       " 273: 'point',\n",
       " 274: 'sure',\n",
       " 275: 'number',\n",
       " 276: 'years',\n",
       " 277: 'second',\n",
       " 278: 'thats',\n",
       " 279: 'knew',\n",
       " 280: 'technology',\n",
       " 281: 'maybe',\n",
       " 282: 'words',\n",
       " 283: 'game',\n",
       " 284: 'enjoy',\n",
       " 285: 'geese',\n",
       " 286: 'stress',\n",
       " 287: 'matter',\n",
       " 288: 'attempting',\n",
       " 289: 'asked',\n",
       " 290: 'days',\n",
       " 291: 'close',\n",
       " 292: 'wind',\n",
       " 293: 'alot',\n",
       " 294: 'felt',\n",
       " 295: 'affected',\n",
       " 296: 'funny',\n",
       " 297: 'public',\n",
       " 298: 'agree',\n",
       " 299: 'fact',\n",
       " 300: 'anyone',\n",
       " 301: 'areas',\n",
       " 302: 'far',\n",
       " 303: 'students',\n",
       " 304: 'minutes',\n",
       " 305: 'open',\n",
       " 306: 'adults',\n",
       " 307: 'three',\n",
       " 308: 'york',\n",
       " 309: 'needed',\n",
       " 310: 'comes',\n",
       " 311: 'frame',\n",
       " 312: 'change',\n",
       " 313: 'town',\n",
       " 314: 'understand',\n",
       " 315: 'car',\n",
       " 316: 'exercise',\n",
       " 317: 'winter',\n",
       " 318: 'brother',\n",
       " 319: 'means',\n",
       " 320: 'happen',\n",
       " 321: 'playing',\n",
       " 322: 'safety',\n",
       " 323: 'hour',\n",
       " 324: 'however',\n",
       " 325: 'saw',\n",
       " 326: 'rather',\n",
       " 327: 'helps',\n",
       " 328: 'plant',\n",
       " 329: 'newspaper',\n",
       " 330: 'violent',\n",
       " 331: 'remember',\n",
       " 332: 'etc',\n",
       " 333: 'coordination',\n",
       " 334: 'highly',\n",
       " 335: 'urban',\n",
       " 336: 'called',\n",
       " 337: 'girl',\n",
       " 338: 'looked',\n",
       " 339: 'kept',\n",
       " 340: 'feeling',\n",
       " 341: 'seen',\n",
       " 342: 'hills',\n",
       " 343: 'learning',\n",
       " 344: 'flammable',\n",
       " 345: 'past',\n",
       " 346: 'flying',\n",
       " 347: 'sister',\n",
       " 348: 'kid',\n",
       " 349: 'hear',\n",
       " 350: 'mind',\n",
       " 351: 'teach',\n",
       " 352: 'airships',\n",
       " 353: 'forget',\n",
       " 354: 'real',\n",
       " 355: 'simple',\n",
       " 356: 'eyes',\n",
       " 357: 'cuban',\n",
       " 358: 'wants',\n",
       " 359: 'lastly',\n",
       " 360: 'research',\n",
       " 361: 'limit',\n",
       " 362: 'remove',\n",
       " 363: 'happened',\n",
       " 364: 'check',\n",
       " 365: 'gave',\n",
       " 366: 'currents',\n",
       " 367: 'write',\n",
       " 368: 'later',\n",
       " 369: 'gives',\n",
       " 370: 'learned',\n",
       " 371: 'realized',\n",
       " 372: 'reader',\n",
       " 373: 'ask',\n",
       " 374: 'safe',\n",
       " 375: 'turned',\n",
       " 376: 'capsnumber',\n",
       " 377: 'censored',\n",
       " 378: 'easy',\n",
       " 379: 'websites',\n",
       " 380: 'constantly',\n",
       " 381: 'sit',\n",
       " 382: 'spending',\n",
       " 383: 'magazine',\n",
       " 384: 'local',\n",
       " 385: 'section',\n",
       " 386: 'everyday',\n",
       " 387: 'strong',\n",
       " 388: 'homework',\n",
       " 389: 'rough',\n",
       " 390: 'class',\n",
       " 391: 'probably',\n",
       " 392: 'phone',\n",
       " 393: 'chat',\n",
       " 394: 'yes',\n",
       " 395: 'writing',\n",
       " 396: 'turn',\n",
       " 397: 'main',\n",
       " 398: 'side',\n",
       " 399: 'man',\n",
       " 400: 'job',\n",
       " 401: 'speed',\n",
       " 402: 'goes',\n",
       " 403: 'leave',\n",
       " 404: 'actually',\n",
       " 405: 'sitting',\n",
       " 406: 'decided',\n",
       " 407: 'parent',\n",
       " 408: 'pick',\n",
       " 409: 'city',\n",
       " 410: 'language',\n",
       " 411: 'front',\n",
       " 412: 'adult',\n",
       " 413: 'easier',\n",
       " 414: 'walk',\n",
       " 415: 'waited',\n",
       " 416: 'food',\n",
       " 417: 'soon',\n",
       " 418: 'desert',\n",
       " 419: 'ready',\n",
       " 420: 'buildings',\n",
       " 421: 'memories',\n",
       " 422: 'along',\n",
       " 423: 'loved',\n",
       " 424: 'excerpt',\n",
       " 425: 'often',\n",
       " 426: 'video',\n",
       " 427: 'nice',\n",
       " 428: 'store',\n",
       " 429: 'lack',\n",
       " 430: 'must',\n",
       " 431: 'lead',\n",
       " 432: 'inside',\n",
       " 433: 'future',\n",
       " 434: 'budding',\n",
       " 435: 'sense',\n",
       " 436: 'sad',\n",
       " 437: 'although',\n",
       " 438: 'heat',\n",
       " 439: 'heard',\n",
       " 440: 'coming',\n",
       " 441: 'true',\n",
       " 442: 'gets',\n",
       " 443: 'warm',\n",
       " 444: 'already',\n",
       " 445: 'effects',\n",
       " 446: 'door',\n",
       " 447: 'paper',\n",
       " 448: 'lost',\n",
       " 449: 'needs',\n",
       " 450: 'relationship',\n",
       " 451: 'yet',\n",
       " 452: 'knowledge',\n",
       " 453: 'simply',\n",
       " 454: 'watching',\n",
       " 455: 'dangerous',\n",
       " 456: 'move',\n",
       " 457: 'buy',\n",
       " 458: 'happiness',\n",
       " 459: 'helium',\n",
       " 460: 'freedom',\n",
       " 461: 'flower',\n",
       " 462: 'situation',\n",
       " 463: 'filled',\n",
       " 464: 'word',\n",
       " 465: 'united',\n",
       " 466: 'takes',\n",
       " 467: 'shifting',\n",
       " 468: 'care',\n",
       " 469: 'negative',\n",
       " 470: 'pass',\n",
       " 471: 'sat',\n",
       " 472: 'across',\n",
       " 473: 'talks',\n",
       " 474: 'return',\n",
       " 475: 'set',\n",
       " 476: 'add',\n",
       " 477: 'allowed',\n",
       " 478: 'thousand',\n",
       " 479: 'offended',\n",
       " 480: 'pretty',\n",
       " 481: 'seeing',\n",
       " 482: 'hurt',\n",
       " 483: 'short',\n",
       " 484: 'fire',\n",
       " 485: 'night',\n",
       " 486: 'least',\n",
       " 487: 'weather',\n",
       " 488: 'behind',\n",
       " 489: 'laughed',\n",
       " 490: 'head',\n",
       " 491: 'communicate',\n",
       " 492: 'loving',\n",
       " 493: 'thinking',\n",
       " 494: 'mad',\n",
       " 495: 'ship',\n",
       " 496: 'growing',\n",
       " 497: 'social',\n",
       " 498: 'huge',\n",
       " 499: 'topic',\n",
       " 500: 'teacher',\n",
       " 501: 'ones',\n",
       " 502: 'architects',\n",
       " 503: 'helpful',\n",
       " 504: 'bring',\n",
       " 505: 'facebook',\n",
       " 506: 'tried',\n",
       " 507: 'order',\n",
       " 508: 'ran',\n",
       " 509: 'trip',\n",
       " 510: 'choice',\n",
       " 511: 'ability',\n",
       " 512: 'name',\n",
       " 513: 'moved',\n",
       " 514: 'either',\n",
       " 515: 'fast',\n",
       " 516: 'project',\n",
       " 517: 'call',\n",
       " 518: 'older',\n",
       " 519: 'feet',\n",
       " 520: 'culture',\n",
       " 521: 'quote',\n",
       " 522: 'pictures',\n",
       " 523: 'benefit',\n",
       " 524: 'essay',\n",
       " 525: 'populated',\n",
       " 526: 'failed',\n",
       " 527: 'walked',\n",
       " 528: 'half',\n",
       " 529: 'small',\n",
       " 530: 'throughout',\n",
       " 531: 'whether',\n",
       " 532: 'land',\n",
       " 533: 'feels',\n",
       " 534: 'greatest',\n",
       " 535: 'group',\n",
       " 536: 'content',\n",
       " 537: 'history',\n",
       " 538: 'illegal',\n",
       " 539: 'experience',\n",
       " 540: 'worse',\n",
       " 541: 'began',\n",
       " 542: 'smile',\n",
       " 543: 'therefore',\n",
       " 544: 'l',\n",
       " 545: 'cool',\n",
       " 546: 'body',\n",
       " 547: 'touch',\n",
       " 548: 'self',\n",
       " 549: 'bike',\n",
       " 550: 'chance',\n",
       " 551: 'living',\n",
       " 552: 'censor',\n",
       " 553: 'flat',\n",
       " 554: 'posted',\n",
       " 555: 'sacrifice',\n",
       " 556: 'skills',\n",
       " 557: 'exceed',\n",
       " 558: 'running',\n",
       " 559: 'became',\n",
       " 560: 'tells',\n",
       " 561: 'grade',\n",
       " 562: 'neighborhood',\n",
       " 563: 'telling',\n",
       " 564: 'families',\n",
       " 565: 'helped',\n",
       " 566: 'gone',\n",
       " 567: 'ahead',\n",
       " 568: 'run',\n",
       " 569: 'everybody',\n",
       " 570: 'usually',\n",
       " 571: 'difficult',\n",
       " 572: 'terrain',\n",
       " 573: 'walking',\n",
       " 574: 'docking',\n",
       " 575: 'seem',\n",
       " 576: 'rest',\n",
       " 577: 'easily',\n",
       " 578: 'inappropriate',\n",
       " 579: 'free',\n",
       " 580: 'choose',\n",
       " 581: 'couple',\n",
       " 582: 'knowing',\n",
       " 583: 'meant',\n",
       " 584: 'jobs',\n",
       " 585: 'meet',\n",
       " 586: 'community',\n",
       " 587: 'affects',\n",
       " 588: 'communication',\n",
       " 589: 'large',\n",
       " 590: 'whatever',\n",
       " 591: 'ideas',\n",
       " 592: 'types',\n",
       " 593: 'working',\n",
       " 594: 'ended',\n",
       " 595: 'issue',\n",
       " 596: 'exercising',\n",
       " 597: 'creates',\n",
       " 598: 'snow',\n",
       " 599: 'worth',\n",
       " 600: 'drop',\n",
       " 601: 'news',\n",
       " 602: 'sun',\n",
       " 603: 'friendship',\n",
       " 604: 'realize',\n",
       " 605: 'brought',\n",
       " 606: 'deal',\n",
       " 607: 'full',\n",
       " 608: 'tired',\n",
       " 609: 'landing',\n",
       " 610: 'grow',\n",
       " 611: 'heart',\n",
       " 612: 'eat',\n",
       " 613: 'view',\n",
       " 614: 'song',\n",
       " 615: 'extremely',\n",
       " 616: 'beginning',\n",
       " 617: 'seemed',\n",
       " 618: 'amount',\n",
       " 619: 'lets',\n",
       " 620: 'lose',\n",
       " 621: 'especially',\n",
       " 622: 'favorite',\n",
       " 623: 'trouble',\n",
       " 624: 'driving',\n",
       " 625: 'kitchen',\n",
       " 626: 'cant',\n",
       " 627: 'question',\n",
       " 628: 'park',\n",
       " 629: 'five',\n",
       " 630: 'california',\n",
       " 631: 'june',\n",
       " 632: 'weight',\n",
       " 633: 'single',\n",
       " 634: 'existing',\n",
       " 635: 'cooking',\n",
       " 636: 'health',\n",
       " 637: 'knows',\n",
       " 638: 'changed',\n",
       " 639: 'boy',\n",
       " 640: 'dehydrated',\n",
       " 641: 'childhood',\n",
       " 642: 'faster',\n",
       " 643: 'hold',\n",
       " 644: 'talked',\n",
       " 645: 'riding',\n",
       " 646: 'showed',\n",
       " 647: 'student',\n",
       " 648: 'passed',\n",
       " 649: 'final',\n",
       " 650: 'men',\n",
       " 651: 'opinions',\n",
       " 652: 'upset',\n",
       " 653: 'upon',\n",
       " 654: 'bit',\n",
       " 655: 'blood',\n",
       " 656: 'considered',\n",
       " 657: 'items',\n",
       " 658: 'fly',\n",
       " 659: 'middle',\n",
       " 660: 'harder',\n",
       " 661: 'article',\n",
       " 662: 'die',\n",
       " 663: 'enjoying',\n",
       " 664: 'search',\n",
       " 665: 'younger',\n",
       " 666: 'personal',\n",
       " 667: 'team',\n",
       " 668: 'teens',\n",
       " 669: 'email',\n",
       " 670: 'drugs',\n",
       " 671: 'third',\n",
       " 672: 'leaving',\n",
       " 673: 'media',\n",
       " 674: 'education',\n",
       " 675: 'seems',\n",
       " 676: 'pressure',\n",
       " 677: 'several',\n",
       " 678: 'four',\n",
       " 679: 'cuba',\n",
       " 680: 'likely',\n",
       " 681: 'week',\n",
       " 682: 'towards',\n",
       " 683: 'text',\n",
       " 684: 'directions',\n",
       " 685: 'authors',\n",
       " 686: 'finds',\n",
       " 687: 'course',\n",
       " 688: 'hands',\n",
       " 689: 'appropriate',\n",
       " 690: 'girls',\n",
       " 691: 'listening',\n",
       " 692: 'blimps',\n",
       " 693: 'rolling',\n",
       " 694: 'amazing',\n",
       " 695: 'uses',\n",
       " 696: 'hit',\n",
       " 697: 'blimp',\n",
       " 698: 'okay',\n",
       " 699: 'pedestrians',\n",
       " 700: 'longer',\n",
       " 701: 'based',\n",
       " 702: 'father',\n",
       " 703: 'spent',\n",
       " 704: 'foot',\n",
       " 705: 'thankful',\n",
       " 706: 'form',\n",
       " 707: 'completely',\n",
       " 708: 'describes',\n",
       " 709: 'support',\n",
       " 710: 'environment',\n",
       " 711: 'major',\n",
       " 712: 'ago',\n",
       " 713: 'beautiful',\n",
       " 714: 'cold',\n",
       " 715: 'picture',\n",
       " 716: 'readers',\n",
       " 717: 'eventually',\n",
       " 718: 'plan',\n",
       " 719: 'website',\n",
       " 720: 'lots',\n",
       " 721: 'caused',\n",
       " 722: 'accident',\n",
       " 723: 'tie',\n",
       " 724: 'stated',\n",
       " 725: 'healthy',\n",
       " 726: 'given',\n",
       " 727: 'entertainment',\n",
       " 728: 'swivel',\n",
       " 729: 'possible',\n",
       " 730: 'smith',\n",
       " 731: 'answer',\n",
       " 732: 'mail',\n",
       " 733: 'pay',\n",
       " 734: 'giving',\n",
       " 735: 'banned',\n",
       " 736: 'held',\n",
       " 737: 'fine',\n",
       " 738: 'lived',\n",
       " 739: 'create',\n",
       " 740: 'cannot',\n",
       " 741: 'speech',\n",
       " 742: 'miles',\n",
       " 743: 'thinks',\n",
       " 744: 'hindenburg',\n",
       " 745: 'melt',\n",
       " 746: 'moving',\n",
       " 747: 'express',\n",
       " 748: 'allows',\n",
       " 749: 'offend',\n",
       " 750: 'screen',\n",
       " 751: 'played',\n",
       " 752: 'thank',\n",
       " 753: 'anymore',\n",
       " 754: 'moment',\n",
       " 755: 'sports',\n",
       " 756: 'journey',\n",
       " 757: 'starting',\n",
       " 758: 'starts',\n",
       " 759: 'dollars',\n",
       " 760: 'sex',\n",
       " 761: 'ten',\n",
       " 762: 'overcome',\n",
       " 763: 'weights',\n",
       " 764: 'purpose',\n",
       " 765: 'al',\n",
       " 766: 'quite',\n",
       " 767: 'schools',\n",
       " 768: 'cousin',\n",
       " 769: 'interesting',\n",
       " 770: 'foundation',\n",
       " 771: 'message',\n",
       " 772: 'interact',\n",
       " 773: 'explains',\n",
       " 774: 'allowing',\n",
       " 775: 'excited',\n",
       " 776: 'mostly',\n",
       " 777: 'jokes',\n",
       " 778: 'sites',\n",
       " 779: 'please',\n",
       " 780: 'ground',\n",
       " 781: 'dinner',\n",
       " 782: 'meaning',\n",
       " 783: 'sign',\n",
       " 784: 'subject',\n",
       " 785: 'greatly',\n",
       " 786: 'written',\n",
       " 787: 'stories',\n",
       " 788: 'overall',\n",
       " 789: 'energy',\n",
       " 790: 'fell',\n",
       " 791: 'quickly',\n",
       " 792: 'loves',\n",
       " 793: 'densely',\n",
       " 794: 'practical',\n",
       " 795: 'save',\n",
       " 796: 'censoring',\n",
       " 797: 'personally',\n",
       " 798: 'violence',\n",
       " 799: 'u',\n",
       " 800: 'alone',\n",
       " 801: 'sentence',\n",
       " 802: 'wont',\n",
       " 803: 'kinds',\n",
       " 804: 'helping',\n",
       " 805: 'closer',\n",
       " 806: 'send',\n",
       " 807: 'stopped',\n",
       " 808: 'steel',\n",
       " 809: 'hill',\n",
       " 810: 'teenagers',\n",
       " 811: 'noticed',\n",
       " 812: 'proud',\n",
       " 813: 'attention',\n",
       " 814: 'perfect',\n",
       " 815: 'becoming',\n",
       " 816: 'changes',\n",
       " 817: 'cultures',\n",
       " 818: 'street',\n",
       " 819: 'members',\n",
       " 820: 'control',\n",
       " 821: 'drive',\n",
       " 822: 'piece',\n",
       " 823: 'useful',\n",
       " 824: 'build',\n",
       " 825: 'e',\n",
       " 826: 'typing',\n",
       " 827: 'rid',\n",
       " 828: 'ends',\n",
       " 829: 'stand',\n",
       " 830: 'worry',\n",
       " 831: 'access',\n",
       " 832: 'showing',\n",
       " 833: 'happens',\n",
       " 834: 'brings',\n",
       " 835: 'birthday',\n",
       " 836: 'entire',\n",
       " 837: 'months',\n",
       " 838: 'known',\n",
       " 839: 'peoples',\n",
       " 840: 'letter',\n",
       " 841: 'im',\n",
       " 842: 'sted',\n",
       " 843: 'guy',\n",
       " 844: 'late',\n",
       " 845: 'moored',\n",
       " 846: 'bed',\n",
       " 847: 'ok',\n",
       " 848: 'race',\n",
       " 849: 'greatful',\n",
       " 850: 'catch',\n",
       " 851: 'fit',\n",
       " 852: 'caring',\n",
       " 853: 'sees',\n",
       " 854: 'case',\n",
       " 855: 'mature',\n",
       " 856: 'teachers',\n",
       " 857: 'decide',\n",
       " 858: 'immigrants',\n",
       " 859: 'likes',\n",
       " 860: 'forever',\n",
       " 861: 'finding',\n",
       " 862: 'joke',\n",
       " 863: 'benefits',\n",
       " 864: 'caught',\n",
       " 865: 'fall',\n",
       " 866: 'feelings',\n",
       " 867: 'onto',\n",
       " 868: 'morning',\n",
       " 869: 'thier',\n",
       " 870: 'countries',\n",
       " 871: 'human',\n",
       " 872: 'study',\n",
       " 873: 'thoughts',\n",
       " 874: 'within',\n",
       " 875: 'vowed',\n",
       " 876: 'doctor',\n",
       " 877: 'met',\n",
       " 878: 'imagine',\n",
       " 879: 'cut',\n",
       " 880: 'issues',\n",
       " 881: 'early',\n",
       " 882: 'sixty',\n",
       " 883: 'exactly',\n",
       " 884: 'arms',\n",
       " 885: 'snake',\n",
       " 886: 'till',\n",
       " 887: 'putting',\n",
       " 888: 'distance',\n",
       " 889: 'kurmaskie',\n",
       " 890: 'strength',\n",
       " 891: 'secondly',\n",
       " 892: 'sight',\n",
       " 893: 'cable',\n",
       " 894: 'business',\n",
       " 895: 'roof',\n",
       " 896: 'disagree',\n",
       " 897: 'impossible',\n",
       " 898: 'yosemite',\n",
       " 899: 'faraway',\n",
       " 900: 'snows',\n",
       " 901: 'travel',\n",
       " 902: 'marcia',\n",
       " 903: 'determined',\n",
       " 904: 'flowers',\n",
       " 905: 'act',\n",
       " 906: 'clearly',\n",
       " 907: 'moods',\n",
       " 908: 'reality',\n",
       " 909: 'begin',\n",
       " 910: 'shop',\n",
       " 911: 'nobody',\n",
       " 912: 'hang',\n",
       " 913: 'bored',\n",
       " 914: 'changing',\n",
       " 915: 'literature',\n",
       " 916: 'views',\n",
       " 917: 'stores',\n",
       " 918: 'somewhere',\n",
       " 919: 'load',\n",
       " 920: 'jersey',\n",
       " 921: 'amidon',\n",
       " 922: 'workers',\n",
       " 923: 'calm',\n",
       " 924: 'modifications',\n",
       " 925: 'lunch',\n",
       " 926: 'whats',\n",
       " 927: 'taught',\n",
       " 928: 'contact',\n",
       " 929: 'silently',\n",
       " 930: 'biggest',\n",
       " 931: 'determination',\n",
       " 932: 'eating',\n",
       " 933: 'table',\n",
       " 934: 'web',\n",
       " 935: 'songs',\n",
       " 936: 'interacting',\n",
       " 937: 'daily',\n",
       " 938: 'didnt',\n",
       " 939: 'addicted',\n",
       " 940: 'reach',\n",
       " 941: 'grew',\n",
       " 942: 'passengers',\n",
       " 943: 'drink',\n",
       " 944: 'break',\n",
       " 945: 'boring',\n",
       " 946: 'guess',\n",
       " 947: 'fresh',\n",
       " 948: 'light',\n",
       " 949: 'serious',\n",
       " 950: 'risk',\n",
       " 951: 'videos',\n",
       " 952: 'character',\n",
       " 953: 'myspace',\n",
       " 954: 'educational',\n",
       " 955: 'hundred',\n",
       " 956: 'tether',\n",
       " 957: 'explain',\n",
       " 958: 'fair',\n",
       " 959: 'fight',\n",
       " 960: 'share',\n",
       " 961: 'wrote',\n",
       " 962: 'wonderful',\n",
       " 963: 'near',\n",
       " 964: 'deserts',\n",
       " 965: 'happening',\n",
       " 966: 'crazy',\n",
       " 967: 'continue',\n",
       " 968: 'removing',\n",
       " 969: 'projects',\n",
       " 970: 'waste',\n",
       " 971: 'somebody',\n",
       " 972: 'ball',\n",
       " 973: 'bus',\n",
       " 974: 'rated',\n",
       " 975: 'looks',\n",
       " 976: 'gas',\n",
       " 977: 'provide',\n",
       " 978: 'gratitude',\n",
       " 979: 'sound',\n",
       " 980: 'office',\n",
       " 981: 'works',\n",
       " 982: 'gotten',\n",
       " 983: 'baby',\n",
       " 984: 'katherine',\n",
       " 985: 'dry',\n",
       " 986: 'weeks',\n",
       " 987: 'party',\n",
       " 988: 'sleep',\n",
       " 989: 'shown',\n",
       " 990: 'laws',\n",
       " 991: 'downtown',\n",
       " 992: 'comfort',\n",
       " 993: 'bought',\n",
       " 994: 'facts',\n",
       " 995: 'ending',\n",
       " 996: 'none',\n",
       " 997: 'dog',\n",
       " 998: 'statement',\n",
       " 999: 'six',\n",
       " 1000: 'paterson',\n",
       " ...}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_y = to_categorical(np.hstack((labels_train, labels_test)))[0:len(labels_train),:]\n",
    "test_y = to_categorical(np.hstack((labels_train, labels_test)))[len(labels_train):,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = w2v[word]\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((5000 + 1, EMBEDDING_DIM))\n",
    "for i in range(1,5001):\n",
    "    word = inv_map[i]\n",
    "    embedding_vector = w2v[word]\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5001, 100)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "sequential_3 (Sequential)        (None, 700, 128)      1084680                                      \n",
      "____________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                    (None, 64)            49408                                        \n",
      "____________________________________________________________________________________________________\n",
      "lstm_4 (LSTM)                    (None, 64)            49408                                        \n",
      "____________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)              (None, 128)           0           merge_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 61)            7869        dropout_2[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 106685\n",
      "____________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-13124a76d7a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_x\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_epoch\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/zelongqiu/anaconda/lib/python2.7/site-packages/keras/models.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, **kwargs)\u001b[0m\n\u001b[1;32m    650\u001b[0m                               \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 652\u001b[0;31m                               sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    653\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m/Users/zelongqiu/anaconda/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch)\u001b[0m\n\u001b[1;32m   1109\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1110\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1111\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/zelongqiu/anaconda/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, nb_epoch, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[1;32m    824\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 826\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    827\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    828\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/zelongqiu/anaconda/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1093\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse_coo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse_coo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1094\u001b[0m             \u001b[0mfeed_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1095\u001b[0;31m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1096\u001b[0m         \u001b[0mupdated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdates_op\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1097\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/zelongqiu/anaconda/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36mget_session\u001b[0;34m()\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_SESSION\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_MANUAL_VAR_INIT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0m_initialize_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/zelongqiu/anaconda/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m_initialize_variables\u001b[0;34m()\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0msess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'variables_initializer'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m             \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muninitialized_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m             \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muninitialized_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/zelongqiu/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 766\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    767\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/zelongqiu/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    962\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 964\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    965\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/zelongqiu/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1014\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1015\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/Users/zelongqiu/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1019\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1020\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1022\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/zelongqiu/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1001\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1002\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1003\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1004\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "embedding_vecor_length = EMBEDDING_DIM\n",
    "\n",
    "def fork (model, n=2):\n",
    "    forks = []\n",
    "    for i in range(n):\n",
    "        f = Sequential()\n",
    "        f.add (model)\n",
    "        forks.append(f)\n",
    "    return forks\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model_left = Sequential()\n",
    "model_left.add(Embedding(5000 + 1, embedding_vecor_length, \\\n",
    "                    weights=[embedding_matrix], input_length=MAX_SEQUENCE_LENGTH, trainable=True))\n",
    "model_left.add(LSTM(64, return_sequences=True))\n",
    "\n",
    "model_right = Sequential()\n",
    "model_right.add(Embedding(5000 + 1, embedding_vecor_length, \\\n",
    "                    weights=[embedding_matrix], input_length=MAX_SEQUENCE_LENGTH, trainable=True))\n",
    "model_right.add(LSTM(64, go_backwards=True, return_sequences=True))\n",
    "model.add( Merge([model_left, model_right], mode='concat'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "left, right = fork(model)\n",
    "\n",
    "left.add(LSTM(64))\n",
    "right.add(LSTM(64,  go_backwards=True))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Merge([left, right], mode='concat'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(61, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit([train_x, train_x], train_y, batch_size = 128, nb_epoch= 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_18 (Embedding)         (None, 500, 300)      10730100    embedding_input_18[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "lstm_19 (LSTM)                   (None, 100)           160400      embedding_18[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)             (None, 100)           0           lstm_19[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_31 (Dense)                 (None, 61)            6161        dropout_10[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 10896661\n",
      "____________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/3\n",
      "10314/10314 [==============================] - 242s - loss: 2.7694 - acc: 0.2224   \n",
      "Epoch 2/3\n",
      "10314/10314 [==============================] - 248s - loss: 2.1013 - acc: 0.3419   \n",
      "Epoch 3/3\n",
      "10314/10314 [==============================] - 243s - loss: 1.7694 - acc: 0.4171   \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1cb3066d0>"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_vecor_length = 300\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1, embedding_vecor_length, \\\n",
    "                    weights=[embedding_matrix], input_length=MAX_SEQUENCE_LENGTH))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(61, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(train_x, train_y, batch_size = 128, nb_epoch= 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1286/1286 [==============================] - 28s    \n"
     ]
    }
   ],
   "source": [
    "pred_y = model.predict_classes([test_x, test_x])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>essay_set</th>\n",
       "      <th>essay</th>\n",
       "      <th>rater1_domain1</th>\n",
       "      <th>rater2_domain1</th>\n",
       "      <th>domain1_score</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear Local Newspaper, @CAPS1 I have found that...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear Local Newspaper, I belive that computers ...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>46</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear, Local Newspaper @CAPS1 here to inform yo...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>55</td>\n",
       "      <td>1</td>\n",
       "      <td>ARE YOU @CAPS1!! Computers are great, they're ...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>61</td>\n",
       "      <td>1</td>\n",
       "      <td>Do you spend all or most of your freetime sitt...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>69</td>\n",
       "      <td>1</td>\n",
       "      <td>Some people think it is a good idea and same d...</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>87</td>\n",
       "      <td>1</td>\n",
       "      <td>Computers. One of the much enjoyed pieces of t...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>108</td>\n",
       "      <td>1</td>\n",
       "      <td>I saw in one of the news papers I got in the m...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>127</td>\n",
       "      <td>1</td>\n",
       "      <td>Computers can affect the way people are and ho...</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>160</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear local newspaper, I agree that people are ...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>165</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear Local Newspaper, everday @CAPS1 technolog...</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>168</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @LOCATION1 Newspaper: @CAPS1 you really t...</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>173</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear, Newspaper I would like to tell you about...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>176</td>\n",
       "      <td>1</td>\n",
       "      <td>Computers and the @CAPS1 were a technological ...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>181</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear Readers of the @ORGANIZATION1, Computers ...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>218</td>\n",
       "      <td>1</td>\n",
       "      <td>@ORGANIZATION1, @CAPS1 you @CAPS2 want your ki...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>247</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear Newspaper @CAPS1, @CAPS2 people are now u...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>263</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @ORGANIZATION1, @CAPS1 minute of the day ...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>275</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @PERSON1, Advances in technology and comp...</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>276</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear local Newspaper, I believe that computers...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>291</td>\n",
       "      <td>1</td>\n",
       "      <td>Have you ever thought about all the amazing th...</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>299</td>\n",
       "      <td>1</td>\n",
       "      <td>Do you think computers have a negative effect ...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>309</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @CAPS1 @CAPS2, @CAPS3 @CAPS4 all the talk...</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>342</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear editor: More and more people use computer...</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>357</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear local newspaper, I am writing to you beca...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>377</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear Readers of the @ORGANIZATION1, @CAPS1 you...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>384</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @CAPS1, @CAPS2 you think computers have a...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>404</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear Local Newspaper, The effects computers ha...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>409</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear local newspaper, @CAPS1 name is @PERSON1....</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>429</td>\n",
       "      <td>1</td>\n",
       "      <td>@ORGANIZATION1, Computers are great tools and ...</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>1462</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear Local Newspaper, @CAPS1 a world where the...</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>1473</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear local news paper, This paper is going to ...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>1489</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear local newspaper, In my opinion I feel lik...</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>1502</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear local newspaper, I believe that with too ...</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>1508</td>\n",
       "      <td>1</td>\n",
       "      <td>Computers. Theres fun. But there's also a down...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>1511</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @ORGANIZATION1, The computer has been a r...</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>1530</td>\n",
       "      <td>1</td>\n",
       "      <td>To: @ORGANIZATION1 goes so fast, and the most ...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>1544</td>\n",
       "      <td>1</td>\n",
       "      <td>I People spend too much time on the computer. ...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>1551</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear local newspaper- I understand that comput...</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>1558</td>\n",
       "      <td>1</td>\n",
       "      <td>@PERCENT1 of all people in the @LOCATION3 have...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>1562</td>\n",
       "      <td>1</td>\n",
       "      <td>The importance of computers in the modern worl...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>1563</td>\n",
       "      <td>1</td>\n",
       "      <td>Computers, one of the daily things we use and ...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>1575</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear Local Newspaper, I am writing to you to t...</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>1578</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear, @LOCATION1 @ORGANIZATION1 I think that a...</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>1594</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @CAPS1, many people say that computers ca...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>1596</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @CAPS1 @CAPS2, I agree that computers had...</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>1603</td>\n",
       "      <td>1</td>\n",
       "      <td>To the local Newspaper, I think using computer...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>1605</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear local newspaper, I think the effects of c...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>1644</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear local newspaper, I strongly believe compu...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>1665</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @CAPS1, Advances in computer technology h...</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>1673</td>\n",
       "      <td>1</td>\n",
       "      <td>Everyone will agree that using computers is al...</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>1703</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @CAPS1 @CAPS2 @CAPS3, The effects compute...</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>1705</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @CAPS1 @CAPS2, @CAPS3, walking into a roo...</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>1717</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @CAPS1, @CAPS2 name is @PERSON1 and I wan...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>1719</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear Local Newspaper, @CAPS1 about what the wo...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>1722</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear local newspaper, @CAPS1 people are talkin...</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>1733</td>\n",
       "      <td>1</td>\n",
       "      <td>I think that computer are a good benefit for p...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>1742</td>\n",
       "      <td>1</td>\n",
       "      <td>To whom it @MONTH1 concern; I am writing this ...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>1763</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear Local Newspaper: @CAPS1 you know that ove...</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>1785</td>\n",
       "      <td>1</td>\n",
       "      <td>My opinion is that people should have computer...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>154 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     essay_id  essay_set                                              essay  \\\n",
       "0           4          1  Dear Local Newspaper, @CAPS1 I have found that...   \n",
       "1          17          1  Dear Local Newspaper, I belive that computers ...   \n",
       "2          46          1  Dear, Local Newspaper @CAPS1 here to inform yo...   \n",
       "3          55          1  ARE YOU @CAPS1!! Computers are great, they're ...   \n",
       "4          61          1  Do you spend all or most of your freetime sitt...   \n",
       "5          69          1  Some people think it is a good idea and same d...   \n",
       "6          87          1  Computers. One of the much enjoyed pieces of t...   \n",
       "7         108          1  I saw in one of the news papers I got in the m...   \n",
       "8         127          1  Computers can affect the way people are and ho...   \n",
       "9         160          1  Dear local newspaper, I agree that people are ...   \n",
       "10        165          1  Dear Local Newspaper, everday @CAPS1 technolog...   \n",
       "11        168          1  Dear @LOCATION1 Newspaper: @CAPS1 you really t...   \n",
       "12        173          1  Dear, Newspaper I would like to tell you about...   \n",
       "13        176          1  Computers and the @CAPS1 were a technological ...   \n",
       "14        181          1  Dear Readers of the @ORGANIZATION1, Computers ...   \n",
       "15        218          1  @ORGANIZATION1, @CAPS1 you @CAPS2 want your ki...   \n",
       "16        247          1  Dear Newspaper @CAPS1, @CAPS2 people are now u...   \n",
       "17        263          1  Dear @ORGANIZATION1, @CAPS1 minute of the day ...   \n",
       "18        275          1  Dear @PERSON1, Advances in technology and comp...   \n",
       "19        276          1  Dear local Newspaper, I believe that computers...   \n",
       "20        291          1  Have you ever thought about all the amazing th...   \n",
       "21        299          1  Do you think computers have a negative effect ...   \n",
       "22        309          1  Dear @CAPS1 @CAPS2, @CAPS3 @CAPS4 all the talk...   \n",
       "23        342          1  Dear editor: More and more people use computer...   \n",
       "24        357          1  Dear local newspaper, I am writing to you beca...   \n",
       "25        377          1  Dear Readers of the @ORGANIZATION1, @CAPS1 you...   \n",
       "26        384          1  Dear @CAPS1, @CAPS2 you think computers have a...   \n",
       "27        404          1  Dear Local Newspaper, The effects computers ha...   \n",
       "28        409          1  Dear local newspaper, @CAPS1 name is @PERSON1....   \n",
       "29        429          1  @ORGANIZATION1, Computers are great tools and ...   \n",
       "..        ...        ...                                                ...   \n",
       "124      1462          1  Dear Local Newspaper, @CAPS1 a world where the...   \n",
       "125      1473          1  Dear local news paper, This paper is going to ...   \n",
       "126      1489          1  Dear local newspaper, In my opinion I feel lik...   \n",
       "127      1502          1  Dear local newspaper, I believe that with too ...   \n",
       "128      1508          1  Computers. Theres fun. But there's also a down...   \n",
       "129      1511          1  Dear @ORGANIZATION1, The computer has been a r...   \n",
       "130      1530          1  To: @ORGANIZATION1 goes so fast, and the most ...   \n",
       "131      1544          1  I People spend too much time on the computer. ...   \n",
       "132      1551          1  Dear local newspaper- I understand that comput...   \n",
       "133      1558          1  @PERCENT1 of all people in the @LOCATION3 have...   \n",
       "134      1562          1  The importance of computers in the modern worl...   \n",
       "135      1563          1  Computers, one of the daily things we use and ...   \n",
       "136      1575          1  Dear Local Newspaper, I am writing to you to t...   \n",
       "137      1578          1  Dear, @LOCATION1 @ORGANIZATION1 I think that a...   \n",
       "138      1594          1  Dear @CAPS1, many people say that computers ca...   \n",
       "139      1596          1  Dear @CAPS1 @CAPS2, I agree that computers had...   \n",
       "140      1603          1  To the local Newspaper, I think using computer...   \n",
       "141      1605          1  Dear local newspaper, I think the effects of c...   \n",
       "142      1644          1  Dear local newspaper, I strongly believe compu...   \n",
       "143      1665          1  Dear @CAPS1, Advances in computer technology h...   \n",
       "144      1673          1  Everyone will agree that using computers is al...   \n",
       "145      1703          1  Dear @CAPS1 @CAPS2 @CAPS3, The effects compute...   \n",
       "146      1705          1  Dear @CAPS1 @CAPS2, @CAPS3, walking into a roo...   \n",
       "147      1717          1  Dear @CAPS1, @CAPS2 name is @PERSON1 and I wan...   \n",
       "148      1719          1  Dear Local Newspaper, @CAPS1 about what the wo...   \n",
       "149      1722          1  Dear local newspaper, @CAPS1 people are talkin...   \n",
       "150      1733          1  I think that computer are a good benefit for p...   \n",
       "151      1742          1  To whom it @MONTH1 concern; I am writing this ...   \n",
       "152      1763          1  Dear Local Newspaper: @CAPS1 you know that ove...   \n",
       "153      1785          1  My opinion is that people should have computer...   \n",
       "\n",
       "     rater1_domain1  rater2_domain1  domain1_score  prediction  \n",
       "0                 5               5             10          12  \n",
       "1                 4               4              8           8  \n",
       "2                 4               4              8           7  \n",
       "3                 4               4              8           8  \n",
       "4                 4               4              8           8  \n",
       "5                 3               4              7           8  \n",
       "6                 5               5             10           9  \n",
       "7                 4               4              8           8  \n",
       "8                 4               5              9           9  \n",
       "9                 4               4              8           8  \n",
       "10                4               5              9           8  \n",
       "11                5               4              9           9  \n",
       "12                4               4              8           8  \n",
       "13                5               5             10          10  \n",
       "14                5               5             10           9  \n",
       "15                4               4              8           8  \n",
       "16                4               4              8           9  \n",
       "17                5               5             10          12  \n",
       "18                6               5             11           9  \n",
       "19                4               4              8           8  \n",
       "20                5               6             11           9  \n",
       "21                4               4              8           8  \n",
       "22                4               5              9           8  \n",
       "23                5               4              9           9  \n",
       "24                4               4              8           7  \n",
       "25                5               5             10           9  \n",
       "26                4               4              8           9  \n",
       "27                4               4              8           9  \n",
       "28                4               4              8           8  \n",
       "29                5               4              9           8  \n",
       "..              ...             ...            ...         ...  \n",
       "124               4               3              7           6  \n",
       "125               3               3              6           6  \n",
       "126               5               4              9           8  \n",
       "127               6               4             10          10  \n",
       "128               4               4              8           8  \n",
       "129               4               3              7          23  \n",
       "130               4               4              8           9  \n",
       "131               5               5             10           9  \n",
       "132               4               5              9          11  \n",
       "133               4               4              8           8  \n",
       "134               2               2              4           6  \n",
       "135               3               3              6           6  \n",
       "136               4               5              9          10  \n",
       "137               4               5              9           9  \n",
       "138               4               4              8           8  \n",
       "139               4               5              9           9  \n",
       "140               4               4              8           8  \n",
       "141               4               4              8           8  \n",
       "142               4               4              8           8  \n",
       "143               4               5              9          10  \n",
       "144               2               3              5           4  \n",
       "145               5               6             11           9  \n",
       "146               6               6             12           8  \n",
       "147               4               4              8          10  \n",
       "148               4               4              8           9  \n",
       "149               4               5              9           8  \n",
       "150               4               4              8           7  \n",
       "151               3               3              6           7  \n",
       "152               6               6             12           9  \n",
       "153               4               4              8           8  \n",
       "\n",
       "[154 rows x 7 columns]"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[\"prediction\"] = pred_y\n",
    "test[test[\"essay_set\"] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kappa for each set [ 0.93582272  0.98486352  0.9729714   0.9837001   0.98912857  0.98136051\n",
      "  0.93398956  0.82689709]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.96812604250544088"
      ]
     },
     "execution_count": 357,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_kappa(train_y, model.predict([train_x, train_x]), train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kappa for each set [ 0.49066455  0.00757581  0.10762456  0.77021246  0.30211683  0.45438115\n",
      "  0.51410443  0.30636055]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.39695948628924149"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_kappa(test_y, model.predict([test_x, test_x]), test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_output(data_df, prediction, name):\n",
    "\tdata_df[\"prediction\"] = prediction\n",
    "\tdata_df = data_df[[\"essay_set\", 'essay_id', 'prediction', 'domain1_score']]\n",
    "\tdata_df.to_csv(name, index = False, header = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1286/1286 [==============================] - 30s    \n"
     ]
    }
   ],
   "source": [
    "generate_output(test, model.predict_classes([test_x, test_x]), 'Bidirectional_lstm_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10314/10314 [==============================] - 234s   \n"
     ]
    }
   ],
   "source": [
    "generate_output(train, model.predict_classes([train_x, train_x]), 'Bidirectional_lstm_train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Sequence Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_NB_WORDS = 5000\n",
    "MAX_SEQUENCE_LENGTH = 400\n",
    "EMBEDDING_DIM = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 35617 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = keras.preprocessing.text.Tokenizer(nb_words = MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(texts_train + texts_test)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_x = tokenizer.texts_to_sequences(texts_train)\n",
    "test_x = tokenizer.texts_to_sequences(texts_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "344"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Shape of data tensor:', (10314, 700))\n"
     ]
    }
   ],
   "source": [
    "train_x = pad_sequences(train_x, padding = 'post',maxlen=MAX_SEQUENCE_LENGTH)\n",
    "test_x = pad_sequences(test_x, padding = 'post', maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "print('Shape of data tensor:', train_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_y = labels_train.astype('float32')\n",
    "test_y = labels_test.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = w2v[word]\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_73 (Embedding)         (None, 400, 100)      3576700                                      \n",
      "____________________________________________________________________________________________________\n",
      "lstm_99 (LSTM)                   (None, 64)            42240                                        \n",
      "____________________________________________________________________________________________________\n",
      "embedding_74 (Embedding)         (None, 400, 100)      3576700                                      \n",
      "____________________________________________________________________________________________________\n",
      "lstm_100 (LSTM)                  (None, 64)            42240                                        \n",
      "____________________________________________________________________________________________________\n",
      "dropout_54 (Dropout)             (None, 128)           0           merge_47[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_59 (Dense)                 (None, 64)            8256        dropout_54[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dropout_55 (Dropout)             (None, 64)            0           dense_59[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_60 (Dense)                 (None, 1)             65          dropout_55[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 7246201\n",
      "____________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/30\n",
      "10314/10314 [==============================] - 215s - loss: 78.5855 - mean_absolute_error: 5.4331   \n",
      "Epoch 2/30\n",
      "10314/10314 [==============================] - 198s - loss: 31.3436 - mean_absolute_error: 3.2365   \n",
      "Epoch 3/30\n",
      "10314/10314 [==============================] - 196s - loss: 13.2848 - mean_absolute_error: 2.1730   \n",
      "Epoch 4/30\n",
      "10314/10314 [==============================] - 198s - loss: 10.6152 - mean_absolute_error: 1.9672   \n",
      "Epoch 5/30\n",
      "10314/10314 [==============================] - 197s - loss: 10.3041 - mean_absolute_error: 1.9201   \n",
      "Epoch 6/30\n",
      "10314/10314 [==============================] - 198s - loss: 7.1891 - mean_absolute_error: 1.6455   \n",
      "Epoch 7/30\n",
      "10314/10314 [==============================] - 192s - loss: 6.2184 - mean_absolute_error: 1.5161   \n",
      "Epoch 8/30\n",
      "10314/10314 [==============================] - 192s - loss: 5.7971 - mean_absolute_error: 1.4273   \n",
      "Epoch 9/30\n",
      "10314/10314 [==============================] - 191s - loss: 4.8290 - mean_absolute_error: 1.3200   \n",
      "Epoch 10/30\n",
      "10314/10314 [==============================] - 192s - loss: 4.5396 - mean_absolute_error: 1.2833   \n",
      "Epoch 11/30\n",
      "10314/10314 [==============================] - 192s - loss: 4.5083 - mean_absolute_error: 1.2494   \n",
      "Epoch 12/30\n",
      "10314/10314 [==============================] - 193s - loss: 4.0557 - mean_absolute_error: 1.2139   \n",
      "Epoch 13/30\n",
      "10314/10314 [==============================] - 208s - loss: 3.9537 - mean_absolute_error: 1.2020   \n",
      "Epoch 14/30\n",
      "10314/10314 [==============================] - 200s - loss: 3.6136 - mean_absolute_error: 1.1433   \n",
      "Epoch 15/30\n",
      "10314/10314 [==============================] - 212s - loss: 3.3530 - mean_absolute_error: 1.1158   \n",
      "Epoch 16/30\n",
      "10314/10314 [==============================] - 206s - loss: 3.2584 - mean_absolute_error: 1.1092   \n",
      "Epoch 17/30\n",
      "10314/10314 [==============================] - 194s - loss: 2.9946 - mean_absolute_error: 1.0742   \n",
      "Epoch 18/30\n",
      "10314/10314 [==============================] - 192s - loss: 2.8704 - mean_absolute_error: 1.0574   \n",
      "Epoch 19/30\n",
      "10314/10314 [==============================] - 195s - loss: 2.9297 - mean_absolute_error: 1.0630   \n",
      "Epoch 20/30\n",
      "10314/10314 [==============================] - 200s - loss: 2.8772 - mean_absolute_error: 1.0469   \n",
      "Epoch 21/30\n",
      "10314/10314 [==============================] - 195s - loss: 2.8786 - mean_absolute_error: 1.0434   \n",
      "Epoch 22/30\n",
      "10314/10314 [==============================] - 197s - loss: 2.6710 - mean_absolute_error: 1.0210   \n",
      "Epoch 23/30\n",
      "10314/10314 [==============================] - 204s - loss: 2.5711 - mean_absolute_error: 0.9851   \n",
      "Epoch 24/30\n",
      "10314/10314 [==============================] - 208s - loss: 2.3691 - mean_absolute_error: 0.9628   \n",
      "Epoch 25/30\n",
      "10314/10314 [==============================] - 198s - loss: 2.2368 - mean_absolute_error: 0.9368   \n",
      "Epoch 26/30\n",
      "10314/10314 [==============================] - 200s - loss: 2.3775 - mean_absolute_error: 0.9752   \n",
      "Epoch 27/30\n",
      "10314/10314 [==============================] - 192s - loss: 2.1824 - mean_absolute_error: 0.9472   \n",
      "Epoch 28/30\n",
      "10314/10314 [==============================] - 192s - loss: 2.1288 - mean_absolute_error: 0.9253   \n",
      "Epoch 29/30\n",
      "10314/10314 [==============================] - 191s - loss: 2.1911 - mean_absolute_error: 0.9195   \n",
      "Epoch 30/30\n",
      "10314/10314 [==============================] - 192s - loss: 1.9635 - mean_absolute_error: 0.8914   \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x3004877d0>"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_vecor_length = EMBEDDING_DIM\n",
    "\n",
    "def fork (model, n=2):\n",
    "    forks = []\n",
    "    for i in range(n):\n",
    "        f = Sequential()\n",
    "        f.add (model)\n",
    "        forks.append(f)\n",
    "    return forks\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model_left = Sequential()\n",
    "model_left.add(Embedding(len(word_index) + 1, embedding_vecor_length, \\\n",
    "                    weights=[embedding_matrix], input_length=MAX_SEQUENCE_LENGTH, trainable=True))\n",
    "model_left.add(LSTM(64))\n",
    "\n",
    "model_right = Sequential()\n",
    "model_right.add(Embedding(len(word_index) + 1, embedding_vecor_length, \\\n",
    "                    weights=[embedding_matrix], input_length=MAX_SEQUENCE_LENGTH, trainable=True))\n",
    "model_right.add(LSTM(64, go_backwards=True))\n",
    "\n",
    "model.add( Merge([model_left, model_right], mode='concat'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(64, activation = 'relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, init='normal'))\n",
    "\n",
    "sgd = SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "\n",
    "\n",
    "model.compile(loss='mean_squared_error', metrics = ['mean_absolute_error'], optimizer='adam')\n",
    "print(model.summary())\n",
    "model.fit([train_x, train_x], train_y, batch_size = 128, nb_epoch= 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_vecor_length = 300\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1, embedding_vecor_length, \\\n",
    "                    weights=[embedding_matrix], input_length=MAX_SEQUENCE_LENGTH))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(13, activation='softmax'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(train_x, train_y, batch_size = 128, nb_epoch= 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(128, 5, activation='relu')(x)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(128, 5, activation='relu')(x)\n",
    "x = MaxPooling1D(35)(x)  # global max pooling\n",
    "x = Flatten()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "preds = Dense(61, activation='softmax')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kappa for each set [ 0.85608191  0.62302006  0.69364329  0.62774886  0.77346977  0.71338742\n",
      "  0.95053852  0.93725111]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.81256575262230601"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regress_kappa(train_y, model.predict([train_x, train_x]), train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kappa for each set [ 0.72805651  0.14926855  0.62803101  0.63152203  0.63041704  0.65642458\n",
      "  0.62133442  0.37867837]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.57262662140342091"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regress_kappa(test_y, model.predict([test_x, test_x]), test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
